{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ACE_vanilla_Code.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMlpwOKunrRyS/WMECdnLDc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jikerbug/jibaek_project_generator/blob/master/ACE_vanilla_Code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xb06XsTxaw7x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import csv\n",
        "import sys\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow.keras as keras\n",
        "import matplotlib.pyplot as plt\n",
        "import os.path\n",
        "\n",
        "\n",
        "def load_data():\n",
        "    input_metadata_chroma_list =[]\n",
        "    output_annotations_chord_list =[]\n",
        "    chord_time_segmentation = []\n",
        "\n",
        "    path_metadata = './metadata/metadata'\n",
        "    csv_metadata = '/bothchroma.csv'\n",
        "    path_annotations = './annotations/annotations/'\n",
        "    csv_annotations = '/majmin.lab'\n",
        "    \n",
        "    for folder_num in range(3,60):\n",
        "        if(folder_num < 10):\n",
        "            path_variable = '/000' + str(folder_num)\n",
        "        elif(folder_num < 100):\n",
        "            path_variable = '/00' + str(folder_num)\n",
        "        elif(folder_num < 1000):\n",
        "            path_variable = '/0' + str(folder_num)\n",
        "        else:\n",
        "            path_variable = '/' + str(folder_num)\n",
        "        \n",
        "        \n",
        "        \n",
        "        complete_path_metadata = path_metadata + path_variable + csv_metadata\n",
        "        complete_path_annotations = path_annotations + path_variable + csv_annotations\n",
        "        \n",
        "        #파일이 존재하는지 체크\n",
        "        if(os.path.isfile(complete_path_metadata)):\n",
        "            pass\n",
        "        else:\n",
        "            continue\n",
        "        \n",
        "        \n",
        "        \n",
        "        with open(complete_path_metadata, 'r', encoding='utf-8') as f:\n",
        "            reader = csv.reader(f)\n",
        "            flag = 0\n",
        "            for segmented_metadata in reader:\n",
        "                input_metadata_chroma_list.append(segmented_metadata)\n",
        "                if flag == 400: # 하나의 곡에서만 너무 많은 학습을 진행시키지 않자 정확도가 급격히 올라간 모습을 볼 수 있었다....!!!무려 54였다...!\n",
        "                    # 혹시 49개를 43개로 고쳤기때문일수도 있겠다\n",
        "                    # 하지만 학습이 진행되도 정확도가 개선되지 않는 큰 문제가 있었다...\n",
        "                    break\n",
        "\n",
        "                flag +=1\n",
        "\n",
        "        with open(complete_path_annotations, 'r', encoding='utf-8') as f:\n",
        "            reader = csv.reader(f)\n",
        "            flag = 0\n",
        "            for segmented_chord_data in reader:\n",
        "\n",
        "\n",
        "\n",
        "                if(segmented_chord_data != []):\n",
        "                    output_annotations_chord_list.append(segmented_chord_data[0].split('\\t')[-1])\n",
        "                    segmentation =[]\n",
        "                    segmentation.append(segmented_chord_data[0].split('\\t')[0])\n",
        "                    segmentation.append(segmented_chord_data[0].split('\\t')[1])\n",
        "\n",
        "                    chord_time_segmentation.append(segmentation)\n",
        "                else:\n",
        "                    break\n",
        "                #\n",
        "                # flag +=1\n",
        "\n",
        "    retyped_chord_to_int_list = []\n",
        "    type1 = ':maj'\n",
        "    type2 = ':min'\n",
        "    type3 = 'b:maj'\n",
        "    type4 = 'b:min'\n",
        "    type5 = '#:maj'\n",
        "    type6 = '#:min'\n",
        "    type_table = [type1, type2, type3, type4, type5, type6]\n",
        "\n",
        "    root_chord_list = ['A', 'B', 'C', 'D', 'E', 'F', 'G']\n",
        "\n",
        "    for chord in output_annotations_chord_list:\n",
        "        escape_flag = False\n",
        "        for chord_num, root_chord in enumerate(root_chord_list):\n",
        "            if (escape_flag):\n",
        "                break\n",
        "            for type_num, type in enumerate(type_table):\n",
        "                if (chord == (root_chord + type)):\n",
        "                    retyped_chord_to_int_list.append(chord_num * 4 + type_num)\n",
        "                    escape_flag = True\n",
        "                    break\n",
        "                elif (chord == 'N' or chord == 'X'):\n",
        "                    retyped_chord_to_int_list.append(42)\n",
        "                    escape_flag = True\n",
        "                    break\n",
        "\n",
        "\n",
        "    #시간 데이터 데이터 삭제\n",
        "    chroma_time_index_list = []\n",
        "    for chroma in input_metadata_chroma_list:\n",
        "        chroma_time_index_list.append(chroma[1])\n",
        "        del chroma[0]\n",
        "        del chroma[0]\n",
        "        #len(chroma) == 24\n",
        "\n",
        "\n",
        "    # string값을 float으로 바꿔서 넣어주기\n",
        "    retyped_chroma_string_to_float = []\n",
        "    for chroma in input_metadata_chroma_list:\n",
        "        retyped_list = []\n",
        "        for value in chroma:\n",
        "            temp_list = []\n",
        "            temp_list.append(float(value))\n",
        "            retyped_list.append(temp_list)\n",
        "        retyped_chroma_string_to_float.append(retyped_list)\n",
        "\n",
        "\n",
        "    print(retyped_chroma_string_to_float[0])\n",
        "    print(chroma_time_index_list[0])\n",
        "    print(chord_time_segmentation[0])\n",
        "    print(retyped_chord_to_int_list[0])\n",
        "\n",
        "\n",
        "    chroma_time_related_chord_to_int_list = []\n",
        "    for chroma_time in chroma_time_index_list:\n",
        "        for time, chord in zip(chord_time_segmentation, retyped_chord_to_int_list):\n",
        "            start = time[0]\n",
        "            end = time[1]\n",
        "            #print(f'chroma_time:{chroma_time} start:{start} end:{end}')\n",
        "            if(float(chroma_time) >= float(start) and float(chroma_time) < float(end)):\n",
        "                chroma_time_related_chord_to_int_list.append(chord)\n",
        "                break\n",
        "\n",
        "\n",
        "\n",
        "    print(\"here\")\n",
        "    print(len(chroma_time_related_chord_to_int_list))\n",
        "    print(len(retyped_chroma_string_to_float))\n",
        "    print(retyped_chroma_string_to_float[0])\n",
        "    print(chroma_time_related_chord_to_int_list[0])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    #n_deleted_output_annotations_chord_list = output_annotations_chord_list\n",
        "    # for chroma, chord in zip(input_metadata_chroma_list, output_annotations_chord_list):\n",
        "    #     print(\"test\" + chord)\n",
        "        \n",
        "    #     if chord == 'N':\n",
        "            \n",
        "    #         n_deleted_input_metadata_chroma_list.remove(chroma)\n",
        "    #         n_deleted_output_annotations_chord_list.remove(chord)\n",
        "    # print(len(retyped_chroma))\n",
        "    # print(len(input_metadata_chroma_list))\n",
        "\n",
        "    for i in output_annotations_chord_list:\n",
        "        print(i)\n",
        "\n",
        "    X = np.array(retyped_chroma_string_to_float)\n",
        "    y = np.array(chroma_time_related_chord_to_int_list)\n",
        "\n",
        "\n",
        "    return X, y\n",
        "\n",
        "#input_data, output_data, segmentation_data = load_data()\n",
        "#print(input_data)\n",
        "#print(output_data)\n",
        "#print(segmentation_data)\n",
        "\n",
        "# for segment in segmentation_data:\n",
        "#     print(float(segment[1]) - float(segment[0]))\n",
        "\n",
        "\n",
        "\n",
        "def plot_history(history):\n",
        "    \"\"\"Plots accuracy/loss for training/validation set as a function of the epochs\n",
        "        :param history: Training history of model\n",
        "        :return:\n",
        "    \"\"\"\n",
        "\n",
        "    fig, axs = plt.subplots(2)\n",
        "\n",
        "    # create accuracy sublpot\n",
        "    axs[0].plot(history.history[\"accuracy\"], label=\"train accuracy\")\n",
        "    axs[0].plot(history.history[\"val_accuracy\"], label=\"test accuracy\")\n",
        "    axs[0].set_ylabel(\"Accuracy\")\n",
        "    axs[0].legend(loc=\"lower right\")\n",
        "    axs[0].set_title(\"Accuracy eval\")\n",
        "\n",
        "    # create error sublpot\n",
        "    axs[1].plot(history.history[\"loss\"], label=\"train error\")\n",
        "    axs[1].plot(history.history[\"val_loss\"], label=\"test error\")\n",
        "    axs[1].set_ylabel(\"Error\")\n",
        "    axs[1].set_xlabel(\"Epoch\")\n",
        "    axs[1].legend(loc=\"upper right\")\n",
        "    axs[1].set_title(\"Error eval\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "def prepare_datasets(test_size, validation_size):\n",
        "\n",
        "\n",
        "    # load data\n",
        "    X, y= load_data()\n",
        "    print(\"Look at here\")\n",
        "    print(len(X))\n",
        "    print(len(y))\n",
        "\n",
        "    # create train/test split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size)\n",
        "\n",
        "    # create train/validation split\n",
        "    X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, test_size=validation_size)\n",
        "\n",
        "    # cnn과 달리 rnn에서는 이러한 3rd 차원이 필요 없다.\n",
        "    # X_train = X_train[..., np.newaxis] # 3d array -> (num_samples = 130, 13, 1)\n",
        "    # X_validation = X_validation[..., np.newaxis] # ... : 기존의 것들\n",
        "    # X_test = X_test[..., np.newaxis]\n",
        "\n",
        "    return X_train, X_validation, X_test, y_train, y_validation, y_test\n",
        "\n",
        "def build_model(input_shape):\n",
        "\n",
        "    # create RNN model\n",
        "    model = keras.Sequential()\n",
        "\n",
        "    # 2 LSTM layers\n",
        "    model.add(keras.layers.LSTM(256, input_shape=input_shape, return_sequences=True))\n",
        "    # return_sequences : second lstm에서 이 시퀀스를 사용하고 싶기 떄문에 true로 한다.\n",
        "    #model.add(keras.layers.LSTM(64, input_shape=input_shape, return_sequences=True)) #레이어를 하나 더 추가하는 것은 큰 효용이 없었다.\n",
        "    model.add(keras.layers.LSTM(256))\n",
        "\n",
        "    # dense layer\n",
        "    model.add(keras.layers.Dense(256, activation='relu'))\n",
        "    model.add(keras.layers.Dropout(0.3))\n",
        "\n",
        "\n",
        "    # output layer\n",
        "    model.add(keras.layers.Dense(43, activation='softmax'))\n",
        "\n",
        "    return model\n",
        "\n",
        "def predict(model, X, y):\n",
        "\n",
        "    X = X[np.newaxis, ...]\n",
        "\n",
        "    # prediction = [[0.1, 0.2, ...]] # softmax의 결과물\n",
        "    prediction = model.predict(X) # X -> (1, 130, 13, 1)\n",
        "\n",
        "    # extract index with max value\n",
        "    predicted_index = np.argmax(prediction, axis=1) # [4]\n",
        "    print(\"Expected index: {}, Predicted index: {}\".format(y, predicted_index))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    pass\n",
        "    # create train, validation and test sets\n",
        "    X_train, X_validation, X_test, y_train, y_validation, y_test = prepare_datasets(0.25, 0.2)\n",
        "\n",
        "    print(len(y_train))\n",
        "\n",
        "    print(y_train[0])\n",
        "    print(y_test[0])\n",
        "    print(y_validation[0])\n",
        "\n",
        "    print(len(X_train))\n",
        "    print(len(X_train[0]))\n",
        "    print(X_train[0])\n",
        "    print(X_test[0])\n",
        "    print(X_validation[0])\n",
        "    #sys.exit()\n",
        "    # create network\n",
        "    print(\"heoollo\")\n",
        "    print(X_train.shape[1],print(X_train.shape[1], ))\n",
        "    input_shape = (X_train.shape[1], X_train.shape[2]) # (130, 13) (number of slices extract mfccs, mfccs)\n",
        "    model = build_model(input_shape)\n",
        "\n",
        "    # compile model\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=0.0001)\n",
        "    model.compile(optimizer=optimizer,\n",
        "                  loss=\"sparse_categorical_crossentropy\",\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    # train model\n",
        "    history = model.fit(X_train, y_train, validation_data=(X_validation, y_validation), batch_size=32, epochs=30)\n",
        "\n",
        "    #plot accuracy/error for training and validation\n",
        "    plot_history(history)\n",
        "\n",
        "    # evaluate model on test set\n",
        "    test_error, test_accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
        "    print(\"Accuracy on test set is: {}\".format(test_accuracy))\n",
        "\n",
        "    # make prediction on a sample\n",
        "    X = X_test[100]\n",
        "    y = y_test[100]\n",
        "\n",
        "\n",
        "    predict(model, X, y)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AI_youtube_code.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN575njEO5Txfp9RQmMiQy7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jikerbug/jibaek_project_generator/blob/master/AI_youtube_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqUOYRVZvZH9",
        "colab_type": "text"
      },
      "source": [
        "#speech recognition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LE6WimGmvVBE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# prepare dataset\n",
        "\n",
        "import librosa\n",
        "import os\n",
        "import json\n",
        "\n",
        "#MFCC (# time steps, # of coefficient)\n",
        "\n",
        "DATASET_PATH = 'data'\n",
        "JSON_PATH = 'data.json'\n",
        "SAMPLES_TO_CONSIDER = 22050 # 1 sec worth of sound sample rate\n",
        "\n",
        "def prepare_dataset(dataset_path, json_path, n_mfcc=13, hop_length=512, n_fft=20):\n",
        "    # hop_length : how big segment should be\n",
        "    data = {\n",
        "        \"mappings\" : [],\n",
        "        \"labels\" : [],\n",
        "        \"MFCCs\" : [],\n",
        "        \"files\" : [],\n",
        "    }\n",
        "\n",
        "    # loop through all the sub-dirs\n",
        "    for i, (dirpath, dirnames, filenames) in enumerate(os.walk(dataset_path)):\n",
        "\n",
        "        # we need to ensure that we`re not at root level\n",
        "        if dirpath is not dataset_path:\n",
        "            # update mappings\n",
        "            category = dirpath.split(\"/\")[-1] # dataset/down -> [dataset, down]\n",
        "            data[\"mappings\"].append(category)\n",
        "            print(f'Processing {category}')\n",
        "            # loop through all the filenames and extract MFCCs\n",
        "\n",
        "            for f in filenames:\n",
        "\n",
        "                # get file path\n",
        "                file_path = os.path.join(dirpath, f)\n",
        "\n",
        "                # load audio file\n",
        "                signal, sr = librosa.load(file_path)\n",
        "\n",
        "                # ensure the audio file is at least 1 sec\n",
        "                if len(signal) >= SAMPLES_TO_CONSIDER: #CNN할때 같은 모양으로 넣어줘야하기 때문에 조건문 체크!\n",
        "\n",
        "                    # enforce 1 sec. long signal\n",
        "                    signal = signal[:SAMPLES_TO_CONSIDER]\n",
        "\n",
        "                    # extract the MFCCs\n",
        "\n",
        "                    MFCCs = librosa.feature.mfcc(signal,n_mfcc=n_mfcc, hop_length=hop_length, n_fft=n_fft)\n",
        "\n",
        "\n",
        "                    # store data\n",
        "                    data[\"labels\"].append(i-1) #first : root folder, second : first sub folder\n",
        "                    data[\"MFCCs\"].append(MFCCs.T.tolist()) #json으로 저장할때 리스트여야함\n",
        "                    data[\"files\"].append(file_path)\n",
        "                    print(f'Processing {file_path}: {i-1}')\n",
        "\n",
        "\n",
        "    with open(json_path, \"w\") as fp:\n",
        "        json.dump(data, fp, indent=4)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    prepare_dataset(DATASET_PATH, JSON_PATH)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TuRRjTLJvtSk",
        "colab_type": "text"
      },
      "source": [
        "#music_genre_classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GT7axEbmvwv2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import music\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import json\n",
        "import os\n",
        "import math\n",
        "import librosa\n",
        "\n",
        "DATASET_PATH = \"C:\\JIBAEK\\genres\"\n",
        "JSON_PATH = \"data.json\"\n",
        "SAMPLE_RATE = 22050\n",
        "TRACK_DURATION = 30  # measured in seconds\n",
        "SAMPLES_PER_TRACK = SAMPLE_RATE * TRACK_DURATION\n",
        "\n",
        "\n",
        "def save_mfcc(dataset_path, json_path, num_mfcc=13, n_fft=2048, hop_length=512, num_segments=5):\n",
        "    \"\"\"Extracts MFCCs from music dataset and saves them into a json file along witgh genre labels.\n",
        "        :param dataset_path (str): Path to dataset\n",
        "        :param json_path (str): Path to json file used to save MFCCs\n",
        "        :param num_mfcc (int): Number of coefficients to extract\n",
        "        :param n_fft (int): Interval we consider to apply FFT. Measured in # of samples\n",
        "        :param hop_length (int): Sliding window for FFT. Measured in # of samples\n",
        "        :param: num_segments (int): Number of segments we want to divide sample tracks into\n",
        "        :return:\n",
        "        \"\"\"\n",
        "\n",
        "    # dictionary to store mapping, labels, and MFCCs\n",
        "    data = {\n",
        "        \"mapping\": [],\n",
        "        \"labels\": [],\n",
        "        \"mfcc\": []\n",
        "    }\n",
        "\n",
        "    samples_per_segment = int(SAMPLES_PER_TRACK / num_segments)\n",
        "    num_mfcc_vectors_per_segment = math.ceil(samples_per_segment / hop_length)\n",
        "    # 이것이 interval의 총 개수이고 각각의 interval 마다 13개의 각각의 mfcc값들이 구해진다.\n",
        "\n",
        "    # loop through all genre sub-folder\n",
        "    for i, (dirpath, dirnames, filenames) in enumerate(os.walk(dataset_path)):\n",
        "\n",
        "        # ensure we're processing a genre sub-folder level\n",
        "        if dirpath is not dataset_path:\n",
        "\n",
        "            # save genre label (i.e., sub-folder name) in the mapping\n",
        "            semantic_label = dirpath.split(\"\\\\\")[-1]\n",
        "            data[\"mapping\"].append(semantic_label)\n",
        "            print(\"\\nProcessing: {}\".format(semantic_label))\n",
        "\n",
        "            # process all audio files in genre sub-dir\n",
        "            for f in filenames:\n",
        "\n",
        "                # load audio file\n",
        "                file_path = os.path.join(dirpath, f)\n",
        "                signal, sample_rate = librosa.load(file_path, sr=SAMPLE_RATE)\n",
        "\n",
        "                # process all segments of audio file\n",
        "                for d in range(num_segments):\n",
        "\n",
        "                    # calculate start and finish sample for current segment\n",
        "                    start = samples_per_segment * d\n",
        "                    finish = start + samples_per_segment\n",
        "\n",
        "                    # extract mfcc\n",
        "                    mfcc = librosa.feature.mfcc(signal[start:finish], sample_rate, n_mfcc=num_mfcc, n_fft=n_fft,\n",
        "                                                hop_length=hop_length)\n",
        "                    mfcc = mfcc.T\n",
        "\n",
        "                    # store only mfcc feature with expected number of vectors\n",
        "                    if len(mfcc) == num_mfcc_vectors_per_segment:\n",
        "                        data[\"mfcc\"].append(mfcc.tolist())\n",
        "                        data[\"labels\"].append(i - 1)\n",
        "                        print(\"{}, segment:{}\".format(file_path, d + 1))\n",
        "\n",
        "    # save MFCCs to json file\n",
        "    with open(json_path, \"w\") as fp:\n",
        "        json.dump(data, fp, indent=4)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    save_mfcc(DATASET_PATH, JSON_PATH, num_segments=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EjwHP1QBv5vJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# lstm\n",
        "\n",
        "import json\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow.keras as keras\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "\n",
        "DATA_PATH = \"data.json\"\n",
        "\n",
        "\n",
        "def plot_history(history):\n",
        "    \"\"\"Plots accuracy/loss for training/validation set as a function of the epochs\n",
        "        :param history: Training history of model\n",
        "        :return:\n",
        "    \"\"\"\n",
        "\n",
        "    fig, axs = plt.subplots(2)\n",
        "\n",
        "    # create accuracy sublpot\n",
        "    axs[0].plot(history.history[\"accuracy\"], label=\"train accuracy\")\n",
        "    axs[0].plot(history.history[\"val_accuracy\"], label=\"test accuracy\")\n",
        "    axs[0].set_ylabel(\"Accuracy\")\n",
        "    axs[0].legend(loc=\"lower right\")\n",
        "    axs[0].set_title(\"Accuracy eval\")\n",
        "\n",
        "    # create error sublpot\n",
        "    axs[1].plot(history.history[\"loss\"], label=\"train error\")\n",
        "    axs[1].plot(history.history[\"val_loss\"], label=\"test error\")\n",
        "    axs[1].set_ylabel(\"Error\")\n",
        "    axs[1].set_xlabel(\"Epoch\")\n",
        "    axs[1].legend(loc=\"upper right\")\n",
        "    axs[1].set_title(\"Error eval\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "def load_data(data_path):\n",
        "    with open(data_path, \"r\") as fp:\n",
        "        data = json.load(fp)\n",
        "\n",
        "    X = np.array(data[\"mfcc\"])\n",
        "    y = np.array(data[\"labels\"])\n",
        "\n",
        "    return X, y\n",
        "\n",
        "def prepare_datasets(test_size, validation_size):\n",
        "\n",
        "    # load data\n",
        "    X, y = load_data(DATA_PATH)\n",
        "\n",
        "    # create train/test split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size)\n",
        "\n",
        "\n",
        "    # create train/validation split\n",
        "    X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, test_size=validation_size)\n",
        "\n",
        "    # cnn과 달리 rnn에서는 이러한 3rd 차원이 필요 없다.\n",
        "    # X_train = X_train[..., np.newaxis] # 3d array -> (num_samples = 130, 13, 1)\n",
        "    # X_validation = X_validation[..., np.newaxis] # ... : 기존의 것들\n",
        "    # X_test = X_test[..., np.newaxis]\n",
        "\n",
        "    return X_train, X_validation, X_test, y_train, y_validation, y_test\n",
        "\n",
        "def build_model(input_shape):\n",
        "\n",
        "    # create RNN model\n",
        "    model = keras.Sequential()\n",
        "\n",
        "    # 2 LSTM layers\n",
        "    model.add(keras.layers.LSTM(64, input_shape=input_shape, return_sequences=True))\n",
        "    # return_sequences : second lstm에서 이 시퀀스를 사용하고 싶기 떄문에 true로 한다.\n",
        "    model.add(keras.layers.LSTM(64))\n",
        "\n",
        "    # dense layer\n",
        "    model.add(keras.layers.Dense(64, activation='relu'))\n",
        "    model.add(keras.layers.Dropout(0.3))\n",
        "\n",
        "\n",
        "    # output layer\n",
        "    model.add(keras.layers.Dense(10, activation='softmax'))\n",
        "\n",
        "    return model\n",
        "\n",
        "def predict(model, X, y):\n",
        "\n",
        "    X = X[np.newaxis, ...]\n",
        "\n",
        "    # prediction = [[0.1, 0.2, ...]] # softmax의 결과물\n",
        "    prediction = model.predict(X) # X -> (1, 130, 13, 1)\n",
        "\n",
        "    # extract index with max value\n",
        "    predicted_index = np.argmax(prediction, axis=1) # [4]\n",
        "    print(\"Expected index: {}, Predicted index: {}\".format(y, predicted_index))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    pass\n",
        "    # create train, validation and test sets\n",
        "    X_train, X_validation, X_test, y_train, y_validation, y_test = prepare_datasets(0.25, 0.2)\n",
        "\n",
        "    print(y_train)\n",
        "    #\n",
        "    print(y_train[0])\n",
        "    # print(y_test[0])\n",
        "    # print(y_validation[0])\n",
        "    #\n",
        "    print(len(X_train))\n",
        "    # print(len(X_train[0]))\n",
        "    # print(X_train[0])\n",
        "    # print(X_test[0])\n",
        "    # print(X_validation[0])\n",
        "\n",
        "    #sys.exit()\n",
        "\n",
        "    # create network\n",
        "\n",
        "    print(X_train)\n",
        "    print(X_train[0])\n",
        "    print(y_train)\n",
        "    print(y_train[0])\n",
        "    input_shape = (\n",
        "    X_train.shape[1], X_train.shape[2])  # (130, 13) (number of slices extract mfccs, mfccs)\n",
        "\n",
        "    print(input_shape)\n",
        "    model = build_model(input_shape)\n",
        "\n",
        "    # compile model\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=0.0001)\n",
        "    model.compile(optimizer=optimizer,\n",
        "                  loss=\"sparse_categorical_crossentropy\",\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    # train model\n",
        "    history = model.fit(X_train, y_train, validation_data=(X_validation, y_validation), batch_size=32, epochs=30)\n",
        "\n",
        "    #plot accuracy/error for training and validation\n",
        "    plot_history(history)\n",
        "\n",
        "    # evaluate model on test set\n",
        "    test_error, test_accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
        "    print(\"Accuracy on test set is: {}\".format(test_accuracy))\n",
        "\n",
        "    # make prediction on a sample\n",
        "    X = X_test[100]\n",
        "    y = y_test[100]\n",
        "\n",
        "\n",
        "    predict(model, X, y)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJEz46tZv_cL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# cnn\n",
        "\n",
        "\n",
        "def prepare_datasets(test_size, validation_size):\n",
        "\n",
        "    # load data\n",
        "    X, y = load_data(DATA_PATH)\n",
        "\n",
        "    # create train/test split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size)\n",
        "\n",
        "    # create train/validation split\n",
        "    X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, test_size=validation_size)\n",
        "\n",
        "    # 3d array -> (130, 13, 1)\n",
        "    X_train = X_train[..., np.newaxis] # 4d array -> (num_samples, 130, 13, 1)\n",
        "    X_validation = X_validation[..., np.newaxis] # ... : 기존의 것들\n",
        "    X_test = X_test[..., np.newaxis]\n",
        "\n",
        "    return X_train, X_validation, X_test, y_train, y_validation, y_test\n",
        "\n",
        "def build_model(input_shape):\n",
        "\n",
        "    print(input_shape)\n",
        "    # create model\n",
        "    model = keras.Sequential()\n",
        "\n",
        "    # 1st conv layer\n",
        "    model.add(keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape)) # grid size, kernel\n",
        "    model.add(keras.layers.MaxPool2D((3,3), strides=(2, 2), padding='same'))\n",
        "    model.add(keras.layers.BatchNormalization())\n",
        "\n",
        "    # 2nd conv layer\n",
        "    model.add(keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))  # grid size, kernel\n",
        "    model.add(keras.layers.MaxPool2D((3, 3), strides=(2, 2), padding='same'))\n",
        "    model.add(keras.layers.BatchNormalization())\n",
        "\n",
        "    # 3rd conv layer\n",
        "    model.add(keras.layers.Conv2D(32, (2, 2), activation='relu', input_shape=input_shape))  # grid size, kernel\n",
        "    model.add(keras.layers.MaxPool2D((2, 2), strides=(2, 2), padding='same'))\n",
        "    model.add(keras.layers.BatchNormalization())\n",
        "\n",
        "    # flatten the output and feed it into dense layer\n",
        "    model.add(keras.layers.Flatten())\n",
        "    model.add(keras.layers.Dense(64, activation='relu'))\n",
        "    model.add(keras.layers.Dropout(0.3))\n",
        "\n",
        "    # output layer\n",
        "    model.add(keras.layers.Dense(10, activation='softmax'))\n",
        "\n",
        "    return model\n",
        "\n",
        "def predict(model, X, y):\n",
        "\n",
        "    X = X[np.newaxis, ...]\n",
        "\n",
        "    # prediction = [[0.1, 0.2, ...]] # softmax의 결과물\n",
        "    prediction = model.predict(X) # X -> (1, 130, 13, 1)\n",
        "\n",
        "    # extract index with max value\n",
        "    predicted_index = np.argmax(prediction, axis=1) # [4]\n",
        "    print(\"Expected index: {}, Predicted index: {}\".format(y, predicted_index))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    pass\n",
        "    # create train, validation and test sets\n",
        "    X_train, X_validation, X_test, y_train, y_validation, y_test = prepare_datasets(0.25, 0.2)\n",
        "\n",
        "    # build the CNN net\n",
        "    input_shape = (X_train.shape[1], X_train.shape[2], X_train.shape[3])\n",
        "    for i in range(1,4):\n",
        "        print(X_train.shape[i])\n",
        "    model = build_model(input_shape)\n",
        "\n",
        "    # compile the network\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=0.0001)\n",
        "    model.compile(optimizer=optimizer,\n",
        "                  loss=\"sparse_categorical_crossentropy\",\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    # train the CNN\n",
        "    model.fit(X_train, y_train, validation_data=(X_validation, y_validation), batch_size=32, epochs=30)\n",
        "\n",
        "    # evaluate the CNN on the test set\n",
        "    test_error, test_accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
        "    print(\"Accuracy on test set is: {}\".format(test_accuracy))\n",
        "\n",
        "    # make prediction on a sample\n",
        "    X = X_test[100]\n",
        "    y = y_test[100]\n",
        "\n",
        "\n",
        "    predict(model, X, y)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oT-QRbEZwFeS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# nn\n",
        "\n",
        "\n",
        "import json\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow.keras as keras\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# path to json file that stores MFCCs and genre labels for each processed segment\n",
        "DATA_PATH = \"data.json\"\n",
        "\n",
        "def load_data(data_path):\n",
        "    \"\"\"Loads training dataset from json file.\n",
        "        :param data_path (str): Path to json file containing data\n",
        "        :return X (ndarray): Inputs\n",
        "        :return y (ndarray): Targets\n",
        "    \"\"\"\n",
        "\n",
        "    with open(data_path, \"r\") as fp:\n",
        "        data = json.load(fp)\n",
        "\n",
        "    # convert lists to numpy arrays\n",
        "    X = np.array(data[\"mfcc\"])\n",
        "    y = np.array(data[\"labels\"])\n",
        "\n",
        "    print(\"Data succesfully loaded!\")\n",
        "\n",
        "    return  X, y\n",
        "\n",
        "def plot_history(history):\n",
        "\n",
        "    fig, axs = plt.subplots(2)\n",
        "\n",
        "    # create accuracy subplot\n",
        "    axs[0].plot(history.history[\"accuracy\"], label=\"train accuracy\")\n",
        "    axs[0].plot(history.history[\"val_accuracy\"], label=\"test accuracy\")\n",
        "    axs[0].set_ylabel(\"Accuracy\")\n",
        "    axs[0].legend(loc=\"lower right\") # matplotlib로 그림을 열심히 그리고 나서,\n",
        "    # “이 색깔의 모양 저것은 무엇이고…” 하는 식으로 설명을 덧붙이면 매우 피곤해집니다.\n",
        "    # 이런 짓을 좀 덜 하려면 legend만 잘 넣어도 됩니다\n",
        "    axs[0].set_title(\"Accuracy eval\")\n",
        "\n",
        "\n",
        "    # create accuracy subplot\n",
        "    axs[1].plot(history.history[\"loss\"], label=\"train error\")\n",
        "    axs[1].plot(history.history[\"val_loss\"], label=\"test error\")\n",
        "    axs[1].set_xlabel(\"Epoch\")\n",
        "    axs[1].set_ylabel(\"Error\")\n",
        "    axs[1].legend(loc=\"upper right\")\n",
        "    axs[1].set_title(\"Error eval\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # load data\n",
        "    X, y = load_data(DATA_PATH)\n",
        "\n",
        "\n",
        "\n",
        "    # create train/test split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
        "\n",
        "    # build network topology\n",
        "    model = keras.Sequential([\n",
        "\n",
        "        # **************오버피팅 해결! -> 복잡한 데이터 : L2 regularization\n",
        "        # **************오버피팅 해결! -> Drop out!\n",
        "\n",
        "        # input layer\n",
        "        keras.layers.Flatten(input_shape=(X.shape[1], X.shape[2])),\n",
        "\n",
        "        # 1st dense layer\n",
        "        keras.layers.Dense(512, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)), # 0.001 : 람다값\n",
        "        keras.layers.Dropout(0.3),\n",
        "\n",
        "        # 2nd dense layer\n",
        "        keras.layers.Dense(256, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)),\n",
        "        keras.layers.Dropout(0.3),\n",
        "\n",
        "        # 3rd dense layer\n",
        "        keras.layers.Dense(64, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)),\n",
        "        keras.layers.Dropout(0.3),\n",
        "\n",
        "        # output layer\n",
        "        keras.layers.Dense(10, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    # compile model\n",
        "    optimiser = keras.optimizers.Adam(learning_rate=0.0001)\n",
        "    model.compile(optimizer=optimiser,\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    # train model\n",
        "    history = model.fit(X_train, y_train, validation_data=(X_test, y_test), batch_size=32, epochs=100) \n",
        "    # batch : 1 Epoch에서 1 iteration, 즉 1회 학습을 할때 읽어들이는 데이터 개수. 총데이터를 채울때까지 iteration을 반복한뒤에 epoch가 끝난다.\n",
        "    plot_history(history)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
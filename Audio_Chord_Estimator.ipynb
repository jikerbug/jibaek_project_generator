{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Audio_Chord_Estimator.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jikerbug/jibaek_project_generator/blob/master/Audio_Chord_Estimator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8uwbPg7KnqYK",
        "colab_type": "text"
      },
      "source": [
        "# 개발 과정\n",
        "1. 인공지능 알고리즘 개념 및 원리 학습\n",
        "2. 코드 데이터 확보\n",
        "3. 딥러닝 모델 생성\n",
        "4. 딥러닝 모델 이용해 음악의 시간별 코드를 출력하는 프로그램 제작\n",
        "5. 시간별 코드를 보여주는 GUI 제작\n",
        "\n",
        "-\n",
        "\n",
        "##**인공지능 모델 1차 개발 일지 ( 2020-07-03 ~ 2020-07-12 )**\n",
        "\n",
        "### 1단계 : 인공지능 알고리즘 개념 및 원리 학습\n",
        "1. 2020-07-03 ~ 2020-07-09 : [Deep Learning (for Audio) with Python ](https://www.youtube.com/playlist?list=PL-wATfeyAMNrtbkCNsLcpoAyBBRJZVlnf) 동영상 강의를 통해 신경망, CNN, RNN, LSTM 개념 학습과 음악 장르 분류 실습 진행\n",
        "        오디오 신호에서 특성 값들을 추출해 신경망에 입력하여 음악장르를 예측하고 분류할 수 있다.\n",
        "\n",
        "\n",
        "### 2단계 : 코드(chord) 데이터 확보\n",
        "1. 2020-07-11 : [McGill-Billboard Songs and Chord Annotations\n",
        "Chord Recognition with Chromagram Data](https://www.kaggle.com/jacobvs/mcgill-billboard)에서 데이터 확보 //주의! : code 설명을 보기전 데이터의 구조 파악 필수\n",
        "        오디오 신호에서 추출한 Chromagram Data라는 24가지 특성 값을 통해 코드를 예측해보자\n",
        "\n",
        "### 3단계 : 딥러닝 모델 생성 및 학습(code 설명)\n",
        "1. 2020-07-11 ~ 2020-07-12 : 결과 : 48개의 코드 분류에 대해 약 31%의 정확도\n",
        "       (Amaj, Amin, Abmaj, Abmin, A#maj, A#min, Bmaj .... G#min, None(코드 없는 audio)) : 총 48개\n",
        "\n",
        "이상 1차 개발 내용은 아래의 코드에서 설명 : //주의! 설명이 다소 부정확할 수 있음\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXn934SBsT1A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import csv # 데이터를 csv 파일에서 불러오기 위함\n",
        "import numpy as np # numpy array로 데이터를 변환하여 선형대수 연산을 하기 위함\n",
        "from sklearn.model_selection import train_test_split # 모델을 학습시킬 training data와 모델의 예측 성능을 검증할 validation data, test data를 나눠주기 위함\n",
        "import tensorflow.keras as keras # 인공지능 모델을 학습시키기 위함\n",
        "import matplotlib.pyplot as plt # 학습된 결과를 그래플 시각화하기 위함\n",
        "import os.path # 존재하지 않는 파일을 불러와서 오류가 나는 경우를 방지하기 위함 (by os.path.isfile(file_path))\n",
        "\n",
        "\n",
        "\n",
        "# 1. 코드 데이터와 를 불러오는 함수\n",
        "def load_data():\n",
        "\n",
        "    #####목표 : \n",
        "    #####1. metadata 폴더에 있는 chromagram 데이터와 해당 chromagram의 시간값 확보 (인공지능 모델의 input : chromagram 데이터. ex : [1.214124, 2.141215, 0.1321, 0, 0, 2.21312, 1.21412 ....] )\n",
        "    #####2. annotations 폴더에 있는 시간별 chord데이터를, chromagram의 시간값과 비교해 chromagram 데이터와 해당 chord값을 맵핑 (인공지능 모델의 output : chord 데이터. ex : 1(A:min이라는 뜻) )\n",
        "    #####3. input과 output을 인공지능 모델에 맞게 가공\n",
        "    \n",
        "    ##1. 중복해서 사용하는 파일 경로를 저장\n",
        "    path_metadata = './metadata/metadata'\n",
        "    csv_metadata = '/bothchroma.csv'\n",
        "    path_annotations = './annotations/annotations/'\n",
        "    csv_annotations = '/majmin.lab'\n",
        "    \n",
        "    input_metadata_chroma_list =[] # csv 파일들에서 Chromagram Data를 전부 받아올 리스트\n",
        "    output_annotations_chord_list =[] # lab 파일들(csv와 똑같이 취급 가능)에서 Chord Data를 전부 받아올 리스트\n",
        "    chord_time_segmentation = [] # 특정 음악의 특정 코드의 시작과 끝 시간을 담을 리스트\n",
        "\n",
        "    for folder_num in range(3,60): #데이터가 너무 많아서 일단은 0003 폴더 부터 0060 폴더 까지의 데이터로만 진행. \n",
        "\n",
        "        ##2. folder_num번째 폴더 파일 경로 저장\n",
        "        path_variable = ''\n",
        "        if(folder_num < 10):\n",
        "            path_variable = '/000' + str(folder_num)\n",
        "        elif(folder_num < 100):\n",
        "            path_variable = '/00' + str(folder_num)\n",
        "        elif(folder_num < 1000):\n",
        "            path_variable = '/0' + str(folder_num)\n",
        "        else:\n",
        "            path_variable = '/' + str(folder_num)\n",
        "        \n",
        "        \n",
        "        ##3. folder_num번째 폴더 파일 경로 저장()\n",
        "        complete_path_metadata = path_metadata + path_variable + csv_metadata # chromagram data\n",
        "        complete_path_annotations = path_annotations + path_variable + csv_annotations # chord data\n",
        "        \n",
        "        ##4. 파일이 존재하는지 체크\n",
        "        if(os.path.isfile(complete_path_metadata)):\n",
        "            pass\n",
        "        else:\n",
        "            continue #존재하지 않으면 아래에서 파일 open하는 과정 생략하고 위로 올라가서 for문 반복\n",
        "        \n",
        "        ##5. Chromagram Data를 불러와서 저장\n",
        "        with open(complete_path_metadata, 'r', encoding='utf-8') as f:\n",
        "            reader = csv.reader(f)\n",
        "            for segmented_metadata in reader:\n",
        "                input_metadata_chroma_list.append(segmented_metadata)\n",
        "\n",
        "       ##6. Chord Data를 불러와서 저장\n",
        "        with open(complete_path_annotations, 'r', encoding='utf-8') as f:\n",
        "            reader = csv.reader(f)\n",
        "            flag = 0\n",
        "            for segmented_chord_data in reader:\n",
        "                if(segmented_chord_data != []):\n",
        "                    output_annotations_chord_list.append(segmented_chord_data[0].split('\\t')[-1])\n",
        "                    segmentation =[]\n",
        "                    segmentation.append(segmented_chord_data[0].split('\\t')[0])\n",
        "                    segmentation.append(segmented_chord_data[0].split('\\t')[1])\n",
        "\n",
        "                    chord_time_segmentation.append(segmentation)\n",
        "                else:\n",
        "                    break\n",
        "                #\n",
        "                # flag +=1\n",
        "\n",
        "    ##7. Chord Data를 0~41까지의 int형 변수로 저장 (7(코드) * 6(코드의 variation) = 42) + none(코드 없는 값) : 48로 설정\n",
        "    #-> 48설정 이유 : 마지막 코드인 G#:min의 int변수 바로 다음인 42로 하기 보다는 결과값의 차이가 좀더 현저하게 나는 48로 설정해야 chord를 none으로 오판하는 일이 적을거라 생각)\n",
        "    retyped_chord_to_int_list = [] #int로 변환된 코드들을 저장\n",
        "    \n",
        "    root_chord_list = ['A', 'B', 'C', 'D', 'E', 'F', 'G']\n",
        "\n",
        "    type1 = ':maj'\n",
        "    type2 = ':min'\n",
        "    type3 = 'b:maj'\n",
        "    type4 = 'b:min'\n",
        "    type5 = '#:maj'\n",
        "    type6 = '#:min'\n",
        "    type_table = [type1, type2, type3, type4, type5, type6]\n",
        "\n",
        "    for chord in output_annotations_chord_list:\n",
        "        escape_flag = False\n",
        "        for chord_num, root_chord in enumerate(root_chord_list):\n",
        "            if (escape_flag):\n",
        "                break\n",
        "            for type_num, type in enumerate(type_table):\n",
        "                if (chord == (root_chord + type)):\n",
        "                    retyped_chord_to_int_list.append(chord_num * 4 + type_num)\n",
        "                    escape_flag = True\n",
        "                    break\n",
        "                elif (chord == 'N' or chord == 'X'):\n",
        "                    retyped_chord_to_int_list.append(48)\n",
        "                    escape_flag = True\n",
        "                    break\n",
        "\n",
        "\n",
        "    ##8. input_metadata_chroma_list의 시간 데이터를 아래의 리스트로 옮기는 데이터 분류작업\n",
        "    chroma_time_index_list = []\n",
        "    for chroma in input_metadata_chroma_list:\n",
        "        chroma_time_index_list.append(chroma[1])\n",
        "        del chroma[0]\n",
        "        del chroma[0]\n",
        "        #len(chroma) == 24\n",
        "\n",
        "\n",
        "    ##9. input_metadata_chroma_list의 string으로 되어있는 숫자값을 float으로 바꿔서 아래의 리스트에 넣어주기 (인공지능 모델에는 float 타입의 chromagram 데이터가 입력되어야 함!)\n",
        "    retyped_chroma_string_to_float = []\n",
        "    for chroma in input_metadata_chroma_list:\n",
        "        retyped_list = []\n",
        "        for value in chroma:\n",
        "            temp_list = []\n",
        "            temp_list.append(float(value))\n",
        "            retyped_list.append(temp_list)\n",
        "        retyped_chroma_string_to_float.append(retyped_list)\n",
        "\n",
        "\n",
        "    # print(retyped_chroma_string_to_float[0])\n",
        "    # print(chroma_time_index_list[0])\n",
        "    # print(chord_time_segmentation[0])\n",
        "    # print(retyped_chord_to_int_list[0])\n",
        "\n",
        "\n",
        "    ##10. chromagram의 시간값(from chroma_time_index_list)이 위치하는 시간대의 chord정보(from chord_time_segmentation(시간대), retyped_chord_to_int_list(chord)))를 아래의 리스트에 저장\n",
        "    chroma_time_related_chord_to_int_list = []\n",
        "    for chroma_time in chroma_time_index_list:\n",
        "        for time, chord in zip(chord_time_segmentation, retyped_chord_to_int_list):\n",
        "            start = time[0]\n",
        "            end = time[1]\n",
        "            #print(f'chroma_time:{chroma_time} start:{start} end:{end}')\n",
        "            if(float(chroma_time) >= float(start) and float(chroma_time) < float(end)):\n",
        "                chroma_time_related_chord_to_int_list.append(chord)\n",
        "                break\n",
        "\n",
        "\n",
        "    # 데이터 구조 파악을 위한 출력함수\n",
        "    # print(\"here\")\n",
        "    # print(len(chroma_time_related_chord_to_int_list))\n",
        "    # print(len(retyped_chroma_string_to_float))\n",
        "    # print(retyped_chroma_string_to_float[0])\n",
        "    # print(chroma_time_related_chord_to_int_list[0])\n",
        "    # for i in output_annotations_chord_list:\n",
        "    #     print(i)\n",
        "\n",
        "    # none 타입을 학습시키지 않기를 원하는 경우 사용할 코드\n",
        "    #n_deleted_output_annotations_chord_list = output_annotations_chord_list\n",
        "    # for chroma, chord in zip(input_metadata_chroma_list, output_annotations_chord_list):\n",
        "    #     print(\"test\" + chord)\n",
        "        \n",
        "    #     if chord == 'N':\n",
        "            \n",
        "    #         n_deleted_input_metadata_chroma_list.remove(chroma)\n",
        "    #         n_deleted_output_annotations_chord_list.remove(chord)\n",
        "    # print(len(retyped_chroma))\n",
        "    # print(len(input_metadata_chroma_list))\n",
        "\n",
        "    ##11. 인공지능 모델 학습을 위해 선형대수 연산이 가능하고 속도가 빠른 numpy array로 변환\n",
        "    X = np.array(retyped_chroma_string_to_float)\n",
        "    y = np.array(chroma_time_related_chord_to_int_list)\n",
        "\n",
        "\n",
        "    return X, y\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 2. 인공지능 모델 학습결과를 시각화하는 함수\n",
        "def plot_history(history):\n",
        "    \"\"\"Plots accuracy/loss for training/validation set as a function of the epochs\n",
        "        :param history: Training history of model\n",
        "        :return:\n",
        "    \"\"\"\n",
        "\n",
        "    fig, axs = plt.subplots(2)\n",
        "\n",
        "    # create accuracy sublpot\n",
        "    axs[0].plot(history.history[\"accuracy\"], label=\"train accuracy\")\n",
        "    axs[0].plot(history.history[\"val_accuracy\"], label=\"test accuracy\")\n",
        "    axs[0].set_ylabel(\"Accuracy\")\n",
        "    axs[0].legend(loc=\"lower right\")\n",
        "    axs[0].set_title(\"Accuracy eval\")\n",
        "\n",
        "    # create error sublpot\n",
        "    axs[1].plot(history.history[\"loss\"], label=\"train error\")\n",
        "    axs[1].plot(history.history[\"val_loss\"], label=\"test error\")\n",
        "    axs[1].set_ylabel(\"Error\")\n",
        "    axs[1].set_xlabel(\"Epoch\")\n",
        "    axs[1].legend(loc=\"upper right\")\n",
        "    axs[1].set_title(\"Error eval\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# 3. 인공지능 모델에 입력할 데이터셋을 test, training, validation으로 구분하는 함수\n",
        "def prepare_datasets(test_size, validation_size):\n",
        "\n",
        "\n",
        "    # load data\n",
        "    X, y= load_data()\n",
        "    print(\"Look at here\")\n",
        "    print(len(X))\n",
        "    print(len(y))\n",
        "\n",
        "    # create train/test split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size)\n",
        "\n",
        "    # create train/validation split\n",
        "    X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, test_size=validation_size)\n",
        "\n",
        "    # cnn과 달리 rnn에서는 이러한 3rd 차원이 필요 없다.\n",
        "    # X_train = X_train[..., np.newaxis] # 3d array -> (num_samples = 130, 13, 1)\n",
        "    # X_validation = X_validation[..., np.newaxis] # ... : 기존의 것들\n",
        "    # X_test = X_test[..., np.newaxis]\n",
        "\n",
        "    return X_train, X_validation, X_test, y_train, y_validation, y_test\n",
        "\n",
        "\n",
        "# 4. 인공지능 모델을 구현하는 함수\n",
        "def build_model(input_shape):\n",
        "\n",
        "    # create RNN model\n",
        "    model = keras.Sequential()\n",
        "\n",
        "    # 2 LSTM layers\n",
        "    model.add(keras.layers.LSTM(256, input_shape=input_shape, return_sequences=True)) #256개의 레이어나 64개의 레이어나 정확도는 약 31%로 똑같았다...\n",
        "    # return_sequences : second lstm에서 이 시퀀스를 사용하고 싶기 떄문에 true로 한다.\n",
        "    model.add(keras.layers.LSTM(256))\n",
        "\n",
        "    # dense layer\n",
        "    model.add(keras.layers.Dense(256, activation='relu'))\n",
        "    model.add(keras.layers.Dropout(0.3))\n",
        "\n",
        "\n",
        "    # output layer\n",
        "    model.add(keras.layers.Dense(49, activation='softmax'))\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# 5. 특정 데이터를 입력하여 결과값을 예측하는 함수\n",
        "def predict(model, X, y):\n",
        "\n",
        "    X = X[np.newaxis, ...]\n",
        "\n",
        "    # prediction = [[0.1, 0.2, ...]] # softmax의 결과물\n",
        "    prediction = model.predict(X) # X -> (1, 130, 13, 1)\n",
        "\n",
        "    # extract index with max value\n",
        "    predicted_index = np.argmax(prediction, axis=1) # [4]\n",
        "    print(\"Expected index: {}, Predicted index: {}\".format(y, predicted_index))\n",
        "\n",
        "# 6. 메인 실행함수\n",
        "if __name__ == \"__main__\":\n",
        "    pass\n",
        "    # create train, validation and test sets\n",
        "    X_train, X_validation, X_test, y_train, y_validation, y_test = prepare_datasets(0.25, 0.2)\n",
        "\n",
        "    # 데이터 구조 파악을 위한 print문\n",
        "    # print(len(y_train))\n",
        "    # print(y_train[0])\n",
        "    # print(y_test[0])\n",
        "    # print(y_validation[0])\n",
        "    # print(len(X_train))\n",
        "    # print(len(X_train[0]))\n",
        "    # print(X_train[0])\n",
        "    # print(X_test[0])\n",
        "    # print(X_validation[0])\n",
        "   \n",
        "    # create network\n",
        "    print(\"heoollo\")\n",
        "    print(X_train.shape[1],print(X_train.shape[1], ))\n",
        "    input_shape = (X_train.shape[1], X_train.shape[2]) # (130, 13) (number of slices extract mfccs, mfccs)\n",
        "    model = build_model(input_shape)\n",
        "\n",
        "    # compile model\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=0.0001)\n",
        "    model.compile(optimizer=optimizer,\n",
        "                  loss=\"sparse_categorical_crossentropy\",\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    # train model\n",
        "    history = model.fit(X_train, y_train, validation_data=(X_validation, y_validation), batch_size=32, epochs=30)\n",
        "\n",
        "    #plot accuracy/error for training and validation\n",
        "    plot_history(history)\n",
        "\n",
        "    # evaluate model on test set\n",
        "    test_error, test_accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
        "    print(\"Accuracy on test set is: {}\".format(test_accuracy))\n",
        "\n",
        "    # make prediction on a sample\n",
        "    X = X_test[100]\n",
        "    y = y_test[100]\n",
        "\n",
        "\n",
        "    predict(model, X, y)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4rVAVeoojj-",
        "colab_type": "text"
      },
      "source": [
        "##인공지능 모델 2차 개발일지 (2020-07-17 ~ 2020-07-18)\n",
        "* 좀더 다양한 음악의 데이터를 넣어보았더니 예측 성능이 50%정도가 나왔다. 그러다가 데이터 범위를 좀더 다양하게 잡아보았는데, 결과가 다시 30%정도로 좋지 않았다. 더 큰 문제는 학습이 진행됨에 따라 정확도가 개선되는 현상이 관측되지 않는다는 것이었다. 아무래도 데이터 입력에 문제가 있어서 인공지능 모델이 제대로 데이터를 학습하지 못하는 것 같았다. 따라서 chromagram data를 묶음으로 만들어줘서 다시 입력해주기로 했다. 다시 살펴보니 지금의 data의 shape는 (24,1)으로 잘못된 형태를 띄고 있었다. 현재 나의 코드는 유튜브 강의 코드를 참고하고 있는데 그곳에서는 (130,13)의 형태를 사용하고 있었다. 그러나 130은 오디오 트따라서 나의 data의 shape를 (130, 24)로 맞춰주는 작업을 진행하기로 했다. \n",
        "\n",
        "* 다시한번 데이터를 살펴보니 코드는 보통 3~4초 사이동안 지속이 되었다. 그러나 하나의 코드를 학습하는 데에 주어진 데이터는 0.05초정도 뿐이었다. 따라서, 나는 코드의 지속시간 평균을 구해본 뒤에 해당 평균 정도의 시간에 맞춰서 데이터를 재구성하여 (50,24)정도의 수준으로 데이터 셋을 구성해보기로 하였다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eG7T6UvJAxV8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import csv\n",
        "import sys\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow.keras as keras\n",
        "import matplotlib.pyplot as plt\n",
        "import os.path\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def load_data():\n",
        "    input_metadata_chroma_list = []\n",
        "    output_annotations_chord_list = []\n",
        "    chord_time_segmentation = []\n",
        "\n",
        "    path_metadata = './metadata/metadata'\n",
        "    csv_metadata = '/bothchroma.csv'\n",
        "    path_annotations = './annotations/annotations/'\n",
        "    csv_annotations = '/majmin.lab'\n",
        "\n",
        "    for folder_num in range(3, 50):\n",
        "        if (folder_num < 10):\n",
        "            path_variable = '/000' + str(folder_num)\n",
        "        elif (folder_num < 100):\n",
        "            path_variable = '/00' + str(folder_num)\n",
        "        elif (folder_num < 1000):\n",
        "            path_variable = '/0' + str(folder_num)\n",
        "        else:\n",
        "            path_variable = '/' + str(folder_num)\n",
        "\n",
        "        complete_path_metadata = path_metadata + path_variable + csv_metadata\n",
        "        complete_path_annotations = path_annotations + path_variable + csv_annotations\n",
        "\n",
        "        # 파일이 존재하는지 체크\n",
        "        if (os.path.isfile(complete_path_metadata)):\n",
        "            pass\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "        with open(complete_path_metadata, 'r', encoding='utf-8') as f:\n",
        "            reader = csv.reader(f)\n",
        "            flag = 0\n",
        "            for _ in range(20):\n",
        "                next(reader)\n",
        "\n",
        "            for segmented_metadata in reader:\n",
        "\n",
        "\n",
        "                input_metadata_chroma_list.append(segmented_metadata)\n",
        "                # 여러 데이터로 시도해본 결과 개선이 이루어지지 않았다. 이것을 보니 데이터의 입력을 수정해야겠다.\n",
        "                # if flag == 50:  # 2차 수정으로 인해 더 많은 데이터를 확보한 결과 정확도가 63까지다시금 더 많이 올라갔다\n",
        "                #     break\n",
        "\n",
        "\n",
        "        with open(complete_path_annotations, 'r', encoding='utf-8') as f:\n",
        "            reader = csv.reader(f)\n",
        "            flag = 0\n",
        "            for segmented_chord_data in reader:\n",
        "\n",
        "                if (segmented_chord_data != []):\n",
        "                    output_annotations_chord_list.append(segmented_chord_data[0].split('\\t')[-1])\n",
        "                    segmentation = []\n",
        "                    segmentation.append(segmented_chord_data[0].split('\\t')[0])\n",
        "                    segmentation.append(segmented_chord_data[0].split('\\t')[1])\n",
        "\n",
        "                    chord_time_segmentation.append(segmentation)\n",
        "                else:\n",
        "                    break\n",
        "                #\n",
        "                # flag +=1\n",
        "\n",
        "    retyped_chord_to_int_list = []\n",
        "    type1 = ':maj'\n",
        "    type2 = ':min'\n",
        "    type3 = 'b:maj'\n",
        "    type4 = 'b:min'\n",
        "    type5 = '#:maj'\n",
        "    type6 = '#:min'\n",
        "    type_table = [type1, type2, type3, type4, type5, type6]\n",
        "\n",
        "    root_chord_list = ['A', 'B', 'C', 'D', 'E', 'F', 'G']\n",
        "\n",
        "    for chord in output_annotations_chord_list:\n",
        "        escape_flag = False\n",
        "        for chord_num, root_chord in enumerate(root_chord_list):\n",
        "            if (escape_flag):\n",
        "                break\n",
        "            for type_num, type in enumerate(type_table):\n",
        "                if (chord == (root_chord + type)):\n",
        "                    retyped_chord_to_int_list.append(chord_num * 6 + type_num)\n",
        "                    escape_flag = True\n",
        "                    break\n",
        "                elif (chord == 'N' or chord == 'X'):\n",
        "                    retyped_chord_to_int_list.append(42)\n",
        "                    escape_flag = True\n",
        "                    break\n",
        "\n",
        "    # 시간 데이터 데이터 삭제\n",
        "    chroma_time_index_list = []\n",
        "    for chroma in input_metadata_chroma_list:\n",
        "        chroma_time_index_list.append(chroma[1])\n",
        "        del chroma[0]\n",
        "        del chroma[0]\n",
        "        # len(chroma) == 24\n",
        "\n",
        "    # string값을 float으로 바꿔서 , (1,24)로 넣어주기\n",
        "    retyped_chroma_string_to_float = []\n",
        "    chroma_for_data_analysis = []\n",
        "    for chroma in input_metadata_chroma_list:\n",
        "        retyped_list = []\n",
        "        for value in chroma:\n",
        "            retyped_list.append(float(value))\n",
        "        dump_list = []\n",
        "        dump_list.append(retyped_list)\n",
        "        chroma_for_data_analysis.append(retyped_list)\n",
        "        retyped_chroma_string_to_float.append(retyped_list) # 단위시간만으로 하고 싶다면 dump_list로 바꾸자\n",
        "\n",
        "    chroma_time_related_chord_to_int_list = []\n",
        "\n",
        "    for chroma_time in chroma_time_index_list:\n",
        "        for time, chord in zip(chord_time_segmentation, retyped_chord_to_int_list):\n",
        "            start = time[0]\n",
        "            end = time[1]\n",
        "            # print(f'chroma_time:{chroma_time} start:{start} end:{end}')\n",
        "            if (float(chroma_time) >= float(start) and float(chroma_time) < float(end)):\n",
        "                chroma_time_related_chord_to_int_list.append(chord)\n",
        "                break\n",
        "\n",
        "    chord_data_changing_idx = []\n",
        "    # idx = 0\n",
        "    for chroma_chord_num in range(len(chroma_time_related_chord_to_int_list) - 1):\n",
        "        if chroma_time_related_chord_to_int_list[chroma_chord_num] != chroma_time_related_chord_to_int_list[\n",
        "            chroma_chord_num + 1]:\n",
        "            chord_data_changing_idx.append(chroma_chord_num + 1)\n",
        "\n",
        "    X = np.array(retyped_chroma_string_to_float)\n",
        "    x_analysis = np.array(chroma_for_data_analysis)\n",
        "    y = np.array(chroma_time_related_chord_to_int_list)\n",
        "\n",
        "    df_x = pd.DataFrame(data=x_analysis, index=None, columns=None)\n",
        "    df_y = pd.DataFrame(data=y, index=None, columns=['chord'])\n",
        "    print(\"this is x data\")\n",
        "    print(df_x.iloc)\n",
        "    print(\"this is y data\")\n",
        "    print(df_y.iloc)\n",
        "\n",
        "    # 각각 코드가 바뀌는 시작점기록\n",
        "    idx = np.array(chord_data_changing_idx)\n",
        "    df_test = pd.DataFrame(data=idx, index=None, columns=None)\n",
        "    print(df_test)\n",
        "\n",
        "    # 시작점의 차이의 정보들 기록\n",
        "    interval_info = []\n",
        "    for i in range(len(chord_data_changing_idx) - 1):\n",
        "        interval_info.append(chord_data_changing_idx[i + 1] - chord_data_changing_idx[i])\n",
        "\n",
        "    interval = np.array(chord_data_changing_idx)\n",
        "    df_interval = pd.DataFrame(data=interval_info, index=None, columns=None)\n",
        "    print(df_interval)\n",
        "    print(df_interval.describe())\n",
        "\n",
        "    chroma_bundle_dataset = []\n",
        "    chord_bundle_dataset = []\n",
        "\n",
        "    for i in range(len(chord_data_changing_idx) - 1):\n",
        "        chord_bundle_dataset.append(chroma_time_related_chord_to_int_list[chord_data_changing_idx[i]])\n",
        "\n",
        "        end_flag = 0\n",
        "        bundle_list = []\n",
        "        for j in range(chord_data_changing_idx[i], chord_data_changing_idx[i + 1]):\n",
        "            bundle_list.append(retyped_chroma_string_to_float[j])\n",
        "            end_flag += 1\n",
        "            if end_flag == 50:\n",
        "                break\n",
        "        for k in range(50 - end_flag):\n",
        "            bundle_list.append(retyped_chroma_string_to_float[chord_data_changing_idx[i]])\n",
        "\n",
        "\n",
        "        chroma_bundle_dataset.append(bundle_list)\n",
        "\n",
        "    # with open('chroma_test.csv', 'w', encoding='utf-8') as f:\n",
        "    #     writer = csv.writer(f)\n",
        "    #     for i in chroma_bundle_dataset:\n",
        "    #         writer.writerow(i)\n",
        "    # with open('chord_test.csv', 'w', encoding='utf-8') as f:\n",
        "    #     writer = csv.writer(f)\n",
        "    #     for i in chord_bundle_dataset:\n",
        "    #         writer.writerow(i)\n",
        "\n",
        "    X_bundle = np.array(chroma_bundle_dataset)\n",
        "    y_bundle = np.array(chord_bundle_dataset)\n",
        "    # print(X_bundle.shape[0])\n",
        "    # print(X_bundle.shape[1])\n",
        "    # print(X_bundle.shape[2])\n",
        "    # print(y_bundle.shape)\n",
        "    # print(chroma_bundle_dataset[3])\n",
        "    # print(chord_bundle_dataset[3])\n",
        "\n",
        "    for i in range(10):\n",
        "        print(retyped_chroma_string_to_float[200 * i][0])\n",
        "        print(chroma_time_related_chord_to_int_list[200*i])\n",
        "    return X_bundle, y_bundle\n",
        "\n",
        "\n",
        "def plot_history(history):\n",
        "    \"\"\"Plots accuracy/loss for training/validation set as a function of the epochs\n",
        "        :param history: Training history of model\n",
        "        :return:\n",
        "    \"\"\"\n",
        "\n",
        "    fig, axs = plt.subplots(2)\n",
        "\n",
        "    # create accuracy sublpot\n",
        "    axs[0].plot(history.history[\"accuracy\"], label=\"train accuracy\")\n",
        "    axs[0].plot(history.history[\"val_accuracy\"], label=\"test accuracy\")\n",
        "    axs[0].set_ylabel(\"Accuracy\")\n",
        "    axs[0].legend(loc=\"lower right\")\n",
        "    axs[0].set_title(\"Accuracy eval\")\n",
        "\n",
        "    # create error sublpot\n",
        "    axs[1].plot(history.history[\"loss\"], label=\"train error\")\n",
        "    axs[1].plot(history.history[\"val_loss\"], label=\"test error\")\n",
        "    axs[1].set_ylabel(\"Error\")\n",
        "    axs[1].set_xlabel(\"Epoch\")\n",
        "    axs[1].legend(loc=\"upper right\")\n",
        "    axs[1].set_title(\"Error eval\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def prepare_datasets(test_size, validation_size):\n",
        "    # load data\n",
        "    X, y = load_data()\n",
        "    print(\"How many dataset : \")\n",
        "    print(len(X))\n",
        "    print(len(y))\n",
        "\n",
        "    # create train/test split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size)\n",
        "\n",
        "    # create train/validation split\n",
        "    X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, test_size=validation_size)\n",
        "\n",
        "    # cnn과 달리 rnn에서는 이러한 3rd 차원이 필요 없다.\n",
        "    # X_train = X_train[..., np.newaxis] # 3d array -> (num_samples = 130, 13, 1)\n",
        "    # X_validation = X_validation[..., np.newaxis] # ... : 기존의 것들\n",
        "    # X_test = X_test[..., np.newaxis]\n",
        "\n",
        "    return X_train, X_validation, X_test, y_train, y_validation, y_test\n",
        "\n",
        "\n",
        "def build_model(input_shape):\n",
        "    # create RNN model\n",
        "    model = keras.Sequential()\n",
        "\n",
        "    # 2 LSTM layers\n",
        "    model.add(keras.layers.LSTM(64, input_shape=input_shape, return_sequences=True))\n",
        "    # return_sequences : second lstm에서 이 시퀀스를 사용하고 싶기 떄문에 true로 한다.\n",
        "    # model.add(keras.layers.LSTM(64, input_shape=input_shape, return_sequences=True)) #레이어를 하나 더 추가하는 것은 큰 효용이 없었다.\n",
        "    model.add(keras.layers.LSTM(64))\n",
        "\n",
        "    # dense layer\n",
        "    model.add(keras.layers.Dense(64, activation='relu'))\n",
        "    model.add(keras.layers.Dropout(0.3))\n",
        "\n",
        "    # output layer\n",
        "    model.add(keras.layers.Dense(43, activation='softmax'))\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def predict(model, X, y):\n",
        "    X = X[np.newaxis, ...]\n",
        "\n",
        "    # prediction = [[0.1, 0.2, ...]] # softmax의 결과물\n",
        "    prediction = model.predict(X)  # X -> (1, 130, 13, 1)\n",
        "\n",
        "    # extract index with max value\n",
        "    predicted_index = np.argmax(prediction, axis=1)  # [4]\n",
        "    print(\"Expected index: {}, Predicted index: {}\".format(y, predicted_index))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    pass\n",
        "    # create train, validation and test sets\n",
        "    X_train, X_validation, X_test, y_train, y_validation, y_test = prepare_datasets(0.25, 0.2)\n",
        "\n",
        "    input_shape = (\n",
        "    X_train.shape[1], X_train.shape[2])  # (130, 13) (number of slices extract mfccs, mfccs) // 여기의 경우에서는 지금은\n",
        "\n",
        "    print(input_shape)\n",
        "    model = build_model(input_shape)\n",
        "\n",
        "    # compile model\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=0.01)\n",
        "    model.compile(optimizer=optimizer,\n",
        "                  loss=\"sparse_categorical_crossentropy\",\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    # train model\n",
        "    history = model.fit(X_train, y_train, validation_data=(X_validation, y_validation), batch_size=32, epochs=30)\n",
        "\n",
        "    # plot accuracy/error for training and validation\n",
        "    plot_history(history)\n",
        "\n",
        "    # evaluate model on test set\n",
        "    test_error, test_accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
        "    print(\"Accuracy on test set is: {}\".format(test_accuracy))\n",
        "\n",
        "    # make prediction on a sample\n",
        "    X = X_test[100]\n",
        "    y = y_test[100]\n",
        "\n",
        "    predict(model, X, y)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOnm0kFFDbfZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###################################### 딕셔너리로 데이터 제대로 정돈하여 약 60%의 예측률 보임! input : (1,24)\n",
        "\n",
        "import csv\n",
        "import sys\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow.keras as keras\n",
        "import matplotlib.pyplot as plt\n",
        "import os.path\n",
        "import pandas as pd\n",
        "\n",
        "#Chromagram (# time steps, # of coefficient)\n",
        "\n",
        "DATASET_PATH = 'data'\n",
        "JSON_PATH = 'data.json'\n",
        "\n",
        "\n",
        "def chord_to_int(chord_name):\n",
        "    type1 = ':maj'\n",
        "    type2 = ':min'\n",
        "    type3 = 'b:maj'\n",
        "    type4 = 'b:min'\n",
        "    type5 = '#:maj'\n",
        "    type6 = '#:min'\n",
        "    type_table = [type1, type2, type3, type4, type5, type6]\n",
        "\n",
        "\n",
        "\n",
        "    root_chord_list = ['A', 'B', 'C', 'D', 'E', 'F', 'G']\n",
        "\n",
        "    if chord_name == 'N' or chord_name == 'X':\n",
        "        return 42\n",
        "    for chord_num, root_chord in enumerate(root_chord_list):\n",
        "        for type_num, type in enumerate(type_table):\n",
        "            if root_chord+type == chord_name:\n",
        "                return chord_num*6 + type_num\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def prepare_dataset(dataset_path, json_path):\n",
        "\n",
        "    data = {\n",
        "        \"labels\": [],\n",
        "        \"Chromagram_bundle\": [],\n",
        "    }\n",
        "\n",
        "    chroma_and_start_time = {\n",
        "        #키값은 각각의 폴더명\n",
        "    }\n",
        "    chord_and_interval_time = {\n",
        "        # 키값은 각각의 폴더명\n",
        "    }\n",
        "\n",
        "    # metadata is for chromagram input\n",
        "    # annotation is for chord output\n",
        "    path_metadata = './metadata/metadata'\n",
        "    file_metadata = '/bothchroma.csv'\n",
        "    path_annotations = './annotations/annotations/'\n",
        "    file_annotations = '/majmin.lab'\n",
        "\n",
        "    for folder_num in range(3, 100):\n",
        "        if (folder_num < 10):\n",
        "            common_path_variable = '/000' + str(folder_num)\n",
        "        elif (folder_num < 100):\n",
        "            common_path_variable = '/00' + str(folder_num)\n",
        "        elif (folder_num < 1000):\n",
        "            common_path_variable = '/0' + str(folder_num)\n",
        "        else:\n",
        "            common_path_variable = '/' + str(folder_num)\n",
        "\n",
        "        complete_path_metadata = path_metadata + common_path_variable + file_metadata\n",
        "        complete_path_annotations = path_annotations + common_path_variable + file_annotations\n",
        "\n",
        "        # 파일이 존재하는지 체크\n",
        "        if (os.path.isfile(complete_path_metadata)):\n",
        "            pass\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "        chroma_and_start_time[str(folder_num)] = []\n",
        "        chord_and_interval_time[str(folder_num)] = []\n",
        "\n",
        "        with open(complete_path_metadata, 'r', encoding='utf-8') as f:\n",
        "            reader = csv.reader(f)\n",
        "            for _ in range(20):\n",
        "                next(reader)\n",
        "\n",
        "            for starting_time_and_chroma in reader:\n",
        "                del starting_time_and_chroma[0]\n",
        "                chroma_and_start_time[str(folder_num)].append(list(map(float, starting_time_and_chroma))) #string을 float으로 저장\n",
        "\n",
        "\n",
        "        with open(complete_path_annotations, 'r', encoding='utf-8') as f:\n",
        "            reader = csv.reader(f)\n",
        "            for interval_time_and_chord in reader:\n",
        "                if (interval_time_and_chord != []):\n",
        "                    chord_data_list = interval_time_and_chord[0].split('\\t')\n",
        "                    chord_data_list[2] = chord_to_int(chord_data_list[2]) #코드이름\n",
        "                    chord_data_list[1] = float(chord_data_list[1])\n",
        "                    chord_data_list[0] = float(chord_data_list[0])\n",
        "                    chord_and_interval_time[str(folder_num)].append(chord_data_list)\n",
        "\n",
        "    print(len(chord_and_interval_time))\n",
        "    print(len(chroma_and_start_time))\n",
        "\n",
        "\n",
        "\n",
        "    for key, value in chroma_and_start_time.items():\n",
        "        for chroma_list in value:\n",
        "            for chord_list in chord_and_interval_time[key]:\n",
        "                if chroma_list[0] >= chord_list[0] and chroma_list[0] < chord_list[1]:\n",
        "                    chroma_list.append(chord_list[2])\n",
        "                    break\n",
        "\n",
        "    for key, value in chroma_and_start_time.items():\n",
        "        for chroma_list in value:\n",
        "            del chroma_list[0]\n",
        "            data[\"labels\"].append(chroma_list[-1])\n",
        "            del chroma_list[-1]\n",
        "            dump_list = []\n",
        "            dump_list.append(chroma_list)\n",
        "            data[\"Chromagram_bundle\"].append(dump_list)\n",
        "\n",
        "    print(len(data[\"labels\"]))\n",
        "    print(data[\"Chromagram_bundle\"][0])\n",
        "    print(len(data[\"Chromagram_bundle\"]))\n",
        "\n",
        "    X = np.array(data[\"Chromagram_bundle\"])\n",
        "    y = np.array(data[\"labels\"])\n",
        "\n",
        "\n",
        "    # for key, value in chroma_and_start_time.items():\n",
        "    #     print(f'key = {key} value = {value}')\n",
        "    #\n",
        "    #\n",
        "    # for key, value in chord_and_interval_time.items():\n",
        "    #     print(f'key = {key} value = {value}')\n",
        "\n",
        "\n",
        "    # chord_changing_index = {}\n",
        "    #\n",
        "    # for key,value in chroma_and_start_time.items():\n",
        "    #     chord_changing_index[key] = []\n",
        "    #     for chord_list_num in range(len(value)-1):\n",
        "    #         if value[chord_list_num][-1] != value[chord_list_num+1][-1]:\n",
        "    #             chord_changing_index[key].append(chord_list_num+1)\n",
        "    #\n",
        "    # print(chord_changing_index)\n",
        "    #\n",
        "    #\n",
        "    # chroma_bundle_dataset = []\n",
        "    # chord_bundle_dataset = []\n",
        "    #\n",
        "    #\n",
        "    # for key, value in chroma_and_start_time.items():\n",
        "    #     pass\n",
        "    # for i in range(len(chord_data_changing_idx) - 1):\n",
        "    #     chord_bundle_dataset.append(chroma_time_related_chord_to_int_list[chord_data_changing_idx[i]])\n",
        "    #\n",
        "    #     end_flag = 0\n",
        "    #     bundle_list = []\n",
        "    #     for j in range(chord_data_changing_idx[i], chord_data_changing_idx[i + 1]):\n",
        "    #         bundle_list.append(retyped_chroma_string_to_float[j])\n",
        "    #         end_flag += 1\n",
        "    #         if end_flag == 50:\n",
        "    #             break\n",
        "    #     for k in range(50 - end_flag):\n",
        "    #         bundle_list.append(retyped_chroma_string_to_float[chord_data_changing_idx[i]])\n",
        "    #\n",
        "    #     chroma_bundle_dataset.append(bundle_list)\n",
        "    #\n",
        "    # return X_bundle, y_bundle\n",
        "    #\n",
        "    #\n",
        "    # data[\"labels\"].append(i - 1)  # first : root folder, second : first sub folder\n",
        "    # data[\"Chromagram\"].append(MFCCs.T.tolist())  # json으로 저장할때 리스트여야함\n",
        "    #\n",
        "    print(X.shape)\n",
        "    print(y.shape)\n",
        "    print(X[0].shape)\n",
        "    return X,y\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def plot_history(history):\n",
        "    \"\"\"Plots accuracy/loss for training/validation set as a function of the epochs\n",
        "        :param history: Training history of model\n",
        "        :return:\n",
        "    \"\"\"\n",
        "\n",
        "    fig, axs = plt.subplots(2)\n",
        "\n",
        "    # create accuracy sublpot\n",
        "    axs[0].plot(history.history[\"accuracy\"], label=\"train accuracy\")\n",
        "    axs[0].plot(history.history[\"val_accuracy\"], label=\"test accuracy\")\n",
        "    axs[0].set_ylabel(\"Accuracy\")\n",
        "    axs[0].legend(loc=\"lower right\")\n",
        "    axs[0].set_title(\"Accuracy eval\")\n",
        "\n",
        "    # create error sublpot\n",
        "    axs[1].plot(history.history[\"loss\"], label=\"train error\")\n",
        "    axs[1].plot(history.history[\"val_loss\"], label=\"test error\")\n",
        "    axs[1].set_ylabel(\"Error\")\n",
        "    axs[1].set_xlabel(\"Epoch\")\n",
        "    axs[1].legend(loc=\"upper right\")\n",
        "    axs[1].set_title(\"Error eval\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def prepare_datasets(test_size, validation_size):\n",
        "    # load data\n",
        "    X, y = prepare_dataset('s', 's')\n",
        "    print(\"How many dataset : \")\n",
        "    print(len(X))\n",
        "    print(len(y))\n",
        "\n",
        "    # create train/test split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size)\n",
        "\n",
        "    # create train/validation split\n",
        "    X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, test_size=validation_size)\n",
        "\n",
        "    # cnn과 달리 rnn에서는 이러한 3rd 차원이 필요 없다.\n",
        "    # X_train = X_train[..., np.newaxis] # 3d array -> (num_samples = 130, 13, 1)\n",
        "    # X_validation = X_validation[..., np.newaxis] # ... : 기존의 것들\n",
        "    # X_test = X_test[..., np.newaxis]\n",
        "\n",
        "    return X_train, X_validation, X_test, y_train, y_validation, y_test\n",
        "\n",
        "\n",
        "def build_model(input_shape):\n",
        "    # create RNN model\n",
        "    model = keras.Sequential()\n",
        "\n",
        "    # 2 LSTM layers\n",
        "    model.add(keras.layers.LSTM(64, input_shape=input_shape, return_sequences=True))\n",
        "    # return_sequences : second lstm에서 이 시퀀스를 사용하고 싶기 떄문에 true로 한다.\n",
        "    # model.add(keras.layers.LSTM(64, input_shape=input_shape, return_sequences=True)) #레이어를 하나 더 추가하는 것은 큰 효용이 없었다.\n",
        "    model.add(keras.layers.LSTM(64))\n",
        "\n",
        "    # dense layer\n",
        "    model.add(keras.layers.Dense(64, activation='relu'))\n",
        "    model.add(keras.layers.Dropout(0.3))\n",
        "\n",
        "    # output layer\n",
        "    model.add(keras.layers.Dense(43, activation='softmax'))\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def predict(model, X, y):\n",
        "    X = X[np.newaxis, ...]\n",
        "\n",
        "    # prediction = [[0.1, 0.2, ...]] # softmax의 결과물\n",
        "    prediction = model.predict(X)  # X -> (1, 130, 13, 1)\n",
        "\n",
        "    # extract index with max value\n",
        "    predicted_index = np.argmax(prediction, axis=1)  # [4]\n",
        "    print(\"Expected index: {}, Predicted index: {}\".format(y, predicted_index))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    pass\n",
        "    # create train, validation and test sets\n",
        "    X_train, X_validation, X_test, y_train, y_validation, y_test = prepare_datasets(0.25, 0.2)\n",
        "\n",
        "    input_shape = (X_train.shape[1], X_train.shape[2])  # (130, 13) (number of slices extract mfccs, mfccs) // 여기의 경우에서는 지금은\n",
        "\n",
        "    print(input_shape)\n",
        "    model = build_model(input_shape)\n",
        "\n",
        "    # compile model\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=0.01)\n",
        "    model.compile(optimizer=optimizer,\n",
        "                  loss=\"sparse_categorical_crossentropy\",\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    # train model\n",
        "    history = model.fit(X_train, y_train, validation_data=(X_validation, y_validation), batch_size=32, epochs=30)\n",
        "\n",
        "    # plot accuracy/error for training and validation\n",
        "    plot_history(history)\n",
        "\n",
        "    # evaluate model on test set\n",
        "    test_error, test_accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
        "    print(\"Accuracy on test set is: {}\".format(test_accuracy))\n",
        "\n",
        "    # make prediction on a sample\n",
        "    X = X_test[100]\n",
        "    y = y_test[100]\n",
        "\n",
        "    predict(model, X, y)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7B0DIVIbDuDJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#######코드 재배열 & input(30,24) 로 성공!!!!!! 80%의 성능!!!!!\n",
        "\n",
        "import csv\n",
        "import sys\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow.keras as keras\n",
        "import matplotlib.pyplot as plt\n",
        "import os.path\n",
        "import pandas as pd\n",
        "\n",
        "#Chromagram (# time steps, # of coefficient)\n",
        "\n",
        "DATASET_PATH = 'data'\n",
        "JSON_PATH = 'data.json'\n",
        "\n",
        "\n",
        "def chord_to_int(chord_name):\n",
        "    type1 = ':maj'\n",
        "    type2 = ':min'\n",
        "    type3 = 'b:maj'\n",
        "    type4 = 'b:min'\n",
        "    type5 = '#:maj'\n",
        "    type6 = '#:min'\n",
        "    type_table = [type1, type2, type3, type4, type5, type6]\n",
        "\n",
        "\n",
        "\n",
        "    root_chord_list = ['A', 'B', 'C', 'D', 'E', 'F', 'G']\n",
        "\n",
        "    if chord_name == 'N' or chord_name == 'X':\n",
        "        return 42\n",
        "    for chord_num, root_chord in enumerate(root_chord_list):\n",
        "        for type_num, type in enumerate(type_table):\n",
        "            if root_chord+type == chord_name:\n",
        "                return chord_num*6 + type_num\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def prepare_dataset(dataset_path, json_path):\n",
        "\n",
        "    data = {\n",
        "        \"labels\": [],\n",
        "        \"Chromagram_bundles\": [],\n",
        "    }\n",
        "\n",
        "    chroma_and_start_time = {\n",
        "        #키값은 각각의 폴더명\n",
        "    }\n",
        "    chord_and_interval_time = {\n",
        "        # 키값은 각각의 폴더명\n",
        "    }\n",
        "\n",
        "    # metadata is for chromagram input\n",
        "    # annotation is for chord output\n",
        "    path_metadata = './metadata/metadata'\n",
        "    file_metadata = '/bothchroma.csv'\n",
        "    path_annotations = './annotations/annotations/'\n",
        "    file_annotations = '/majmin.lab'\n",
        "\n",
        "    for folder_num in range(3, 200):\n",
        "        if (folder_num < 10):\n",
        "            common_path_variable = '/000' + str(folder_num)\n",
        "        elif (folder_num < 100):\n",
        "            common_path_variable = '/00' + str(folder_num)\n",
        "        elif (folder_num < 1000):\n",
        "            common_path_variable = '/0' + str(folder_num)\n",
        "        else:\n",
        "            common_path_variable = '/' + str(folder_num)\n",
        "\n",
        "        complete_path_metadata = path_metadata + common_path_variable + file_metadata\n",
        "        complete_path_annotations = path_annotations + common_path_variable + file_annotations\n",
        "\n",
        "        # 파일이 존재하는지 체크\n",
        "        if (os.path.isfile(complete_path_metadata)):\n",
        "            pass\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "        chroma_and_start_time[str(folder_num)] = []\n",
        "        chord_and_interval_time[str(folder_num)] = []\n",
        "\n",
        "        with open(complete_path_metadata, 'r', encoding='utf-8') as f:\n",
        "            reader = csv.reader(f)\n",
        "            for _ in range(20):\n",
        "                next(reader)\n",
        "\n",
        "            for starting_time_and_chroma in reader:\n",
        "                del starting_time_and_chroma[0]\n",
        "                chroma_and_start_time[str(folder_num)].append(list(map(float, starting_time_and_chroma))) #string을 float으로 저장\n",
        "\n",
        "\n",
        "        with open(complete_path_annotations, 'r', encoding='utf-8') as f:\n",
        "            reader = csv.reader(f)\n",
        "            for interval_time_and_chord in reader:\n",
        "                if (interval_time_and_chord != []):\n",
        "                    chord_data_list = interval_time_and_chord[0].split('\\t')\n",
        "                    chord_data_list[2] = chord_to_int(chord_data_list[2]) #코드이름\n",
        "                    chord_data_list[1] = float(chord_data_list[1])\n",
        "                    chord_data_list[0] = float(chord_data_list[0])\n",
        "                    chord_and_interval_time[str(folder_num)].append(chord_data_list)\n",
        "\n",
        "    print(len(chord_and_interval_time))\n",
        "    print(len(chroma_and_start_time))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    for key, value in chroma_and_start_time.items():\n",
        "\n",
        "        for chord_list in chord_and_interval_time[key]:\n",
        "            chroma_bundle = []\n",
        "            num_flag = 0\n",
        "            for chroma_list in value:\n",
        "                #print(f'chroma_list[0] : {chroma_list[0]}')\n",
        "                #print(f'chord_list[0] : {chord_list[0]}')\n",
        "                #print(f'chord_list[1] : {chord_list[1]}')\n",
        "                if num_flag >= 30:\n",
        "                    break\n",
        "                if chroma_list[0] >= chord_list[1]:\n",
        "                    break\n",
        "                if chroma_list[0] >= chord_list[0] and chroma_list[0] < chord_list[1]:\n",
        "                    del chroma_list[0]\n",
        "                    chroma_bundle.append(chroma_list)\n",
        "                    #print(f'append {chroma_list}')\n",
        "                    value.remove(chroma_list)\n",
        "                    num_flag += 1\n",
        "\n",
        "            if chroma_bundle != []:\n",
        "                for i in range(30-num_flag):\n",
        "                    chroma_bundle.append(chroma_bundle[-1])\n",
        "                if len(chroma_bundle) != 30:\n",
        "                    print(\"문제 발생!!!\")\n",
        "                data[\"Chromagram_bundles\"].append(chroma_bundle)\n",
        "                data[\"labels\"].append(chord_list[2])\n",
        "\n",
        "\n",
        "\n",
        "    print(len(data[\"labels\"]))\n",
        "    print(data[\"Chromagram_bundles\"][0])\n",
        "    print(len(data[\"Chromagram_bundles\"]))\n",
        "\n",
        "    print(len(data[\"Chromagram_bundles\"]))\n",
        "    print(len(data[\"Chromagram_bundles\"][0]))\n",
        "    print(len(data[\"Chromagram_bundles\"][0][0]))\n",
        "\n",
        "    x_list = data[\"Chromagram_bundles\"]\n",
        "    X = np.array(x_list)\n",
        "    y = np.array(data[\"labels\"])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    print(X.shape)\n",
        "    # numpy array의 shape가 분명 (N, 30, 24)로 나와야 하는데 (N, )로 나오는 경우때문에 조금 헤맸다.\n",
        "    # 이 경우는 데이터의 크기가 30,24를 벗어나 있는 경우가 존재한다는 뜻이므로 데이터를 다시 재구성해주어야 한다.\n",
        "    print(y.shape)\n",
        "    return X,y\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def plot_history(history):\n",
        "    \"\"\"Plots accuracy/loss for training/validation set as a function of the epochs\n",
        "        :param history: Training history of model\n",
        "        :return:\n",
        "    \"\"\"\n",
        "\n",
        "    fig, axs = plt.subplots(2)\n",
        "\n",
        "    # create accuracy sublpot\n",
        "    axs[0].plot(history.history[\"accuracy\"], label=\"train accuracy\")\n",
        "    axs[0].plot(history.history[\"val_accuracy\"], label=\"test accuracy\")\n",
        "    axs[0].set_ylabel(\"Accuracy\")\n",
        "    axs[0].legend(loc=\"lower right\")\n",
        "    axs[0].set_title(\"Accuracy eval\")\n",
        "\n",
        "    # create error sublpot\n",
        "    axs[1].plot(history.history[\"loss\"], label=\"train error\")\n",
        "    axs[1].plot(history.history[\"val_loss\"], label=\"test error\")\n",
        "    axs[1].set_ylabel(\"Error\")\n",
        "    axs[1].set_xlabel(\"Epoch\")\n",
        "    axs[1].legend(loc=\"upper right\")\n",
        "    axs[1].set_title(\"Error eval\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def prepare_datasets(test_size, validation_size):\n",
        "    # load data\n",
        "    X, y = prepare_dataset('s', 's')\n",
        "    print(\"How many dataset : \")\n",
        "    print(len(X))\n",
        "    print(len(y))\n",
        "\n",
        "    # create train/test split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size)\n",
        "\n",
        "    # create train/validation split\n",
        "    X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, test_size=validation_size)\n",
        "\n",
        "    # cnn과 달리 rnn에서는 이러한 3rd 차원이 필요 없다.\n",
        "    # X_train = X_train[..., np.newaxis] # 3d array -> (num_samples = 130, 13, 1)\n",
        "    # X_validation = X_validation[..., np.newaxis] # ... : 기존의 것들\n",
        "    # X_test = X_test[..., np.newaxis]\n",
        "\n",
        "    return X_train, X_validation, X_test, y_train, y_validation, y_test\n",
        "\n",
        "\n",
        "def build_model(input_shape):\n",
        "    # create RNN model\n",
        "    model = keras.Sequential()\n",
        "\n",
        "    # 2 LSTM layers\n",
        "    model.add(keras.layers.LSTM(64, input_shape=input_shape, return_sequences=True))\n",
        "    # return_sequences : second lstm에서 이 시퀀스를 사용하고 싶기 떄문에 true로 한다.\n",
        "    # model.add(keras.layers.LSTM(64, input_shape=input_shape, return_sequences=True)) #레이어를 하나 더 추가하는 것은 큰 효용이 없었다.\n",
        "    model.add(keras.layers.LSTM(64))\n",
        "\n",
        "    # dense layer\n",
        "    model.add(keras.layers.Dense(64, activation='relu'))\n",
        "    model.add(keras.layers.Dropout(0.3))\n",
        "\n",
        "    # output layer\n",
        "    model.add(keras.layers.Dense(43, activation='softmax'))\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def predict(model, X, y):\n",
        "    X = X[np.newaxis, ...]\n",
        "\n",
        "    # prediction = [[0.1, 0.2, ...]] # softmax의 결과물\n",
        "    prediction = model.predict(X)  # X -> (1, 130, 13, 1)\n",
        "\n",
        "    # extract index with max value\n",
        "    predicted_index = np.argmax(prediction, axis=1)  # [4]\n",
        "    print(\"Expected index: {}, Predicted index: {}\".format(y, predicted_index))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    pass\n",
        "    # create train, validation and test sets\n",
        "    X_train, X_validation, X_test, y_train, y_validation, y_test = prepare_datasets(0.25, 0.2)\n",
        "\n",
        "    input_shape = (X_train.shape[1], X_train.shape[2])  # (130, 13) (number of slices extract mfccs, mfccs) // 여기의 경우에서는 지금은\n",
        "\n",
        "    print(input_shape)\n",
        "    model = build_model(input_shape)\n",
        "\n",
        "    # compile model\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=0.01)\n",
        "    model.compile(optimizer=optimizer,\n",
        "                  loss=\"sparse_categorical_crossentropy\",\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    # train model\n",
        "    history = model.fit(X_train, y_train, validation_data=(X_validation, y_validation), batch_size=32, epochs=30)\n",
        "\n",
        "    # plot accuracy/error for training and validation\n",
        "    plot_history(history)\n",
        "\n",
        "    # evaluate model on test set\n",
        "    test_error, test_accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
        "    print(\"Accuracy on test set is: {}\".format(test_accuracy))\n",
        "\n",
        "    # make prediction on a sample\n",
        "    X = X_test[100]\n",
        "    y = y_test[100]\n",
        "\n",
        "    predict(model, X, y)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRYwpeFk_7yZ",
        "colab_type": "text"
      },
      "source": [
        "#인공지능 3차 개발일지(2020-07-23 ~ 2020-07-24)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePr0CKWUACMi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### prepare_dataset\n",
        "\n",
        "\n",
        "\n",
        "import csv\n",
        "import numpy as np\n",
        "import os.path\n",
        "import json\n",
        "\n",
        "DATASET_PATH = 'data'\n",
        "JSON_PATH = 'data.json'\n",
        "\n",
        "\n",
        "def chord_to_int(chord_name):\n",
        "    type1 = ':maj'\n",
        "    type2 = ':min'\n",
        "    type3 = 'b:maj'\n",
        "    type4 = 'b:min'\n",
        "    type5 = '#:maj'\n",
        "    type6 = '#:min'\n",
        "    type7 = ':maj7'\n",
        "    type8 = ':min7'\n",
        "    type9 = 'b:maj7'\n",
        "    type10 = 'b:min7'\n",
        "    type11 = '#:maj7'\n",
        "    type12 = '#:min7'\n",
        "    type13 = ':7'\n",
        "    type14 = 'b:7'\n",
        "    type15 = '#:7'\n",
        "    type_table = [type1, type2, type3, type4, type5, type6, type7, type8, type9, type10, type11, type12, type13, type14, type15]\n",
        "\n",
        "    root_chord_list = ['A', 'B', 'C', 'D', 'E', 'F', 'G']\n",
        "\n",
        "    if chord_name == 'N' or chord_name == 'X':\n",
        "        return 105\n",
        "    for chord_num, root_chord in enumerate(root_chord_list):\n",
        "        for type_num, type in enumerate(type_table):\n",
        "            if root_chord + type == chord_name:\n",
        "                return chord_num * 15 + type_num\n",
        "\n",
        "\n",
        "def prepare_dataset(dataset_path, json_path):\n",
        "    data = {\n",
        "        \"labels\": [],\n",
        "        \"Chromagram_bundles\": [],\n",
        "    }\n",
        "\n",
        "    chroma_and_start_time = {\n",
        "        # 키값은 각각의 폴더명\n",
        "    }\n",
        "    chord_and_interval_time = {\n",
        "        # 키값은 각각의 폴더명\n",
        "    }\n",
        "\n",
        "    # metadata is for chromagram input\n",
        "    # annotation is for chord output\n",
        "    path_metadata = './data/metadata/metadata'\n",
        "    file_metadata = '/bothchroma.csv'\n",
        "    path_annotations = './data/annotations/annotations/'\n",
        "    file_annotations = '/majmin.lab'\n",
        "\n",
        "    for folder_num in range(3, 1301):\n",
        "        if (folder_num < 10):\n",
        "            common_path_variable = '/000' + str(folder_num)\n",
        "        elif (folder_num < 100):\n",
        "            common_path_variable = '/00' + str(folder_num)\n",
        "        elif (folder_num < 1000):\n",
        "            common_path_variable = '/0' + str(folder_num)\n",
        "        else:\n",
        "            common_path_variable = '/' + str(folder_num)\n",
        "\n",
        "        complete_path_metadata = path_metadata + common_path_variable + file_metadata\n",
        "        complete_path_annotations = path_annotations + common_path_variable + file_annotations\n",
        "\n",
        "        # 파일이 존재하는지 체크\n",
        "        if (os.path.isfile(complete_path_metadata)):\n",
        "            pass\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "        chroma_and_start_time[str(folder_num)] = []\n",
        "        chord_and_interval_time[str(folder_num)] = []\n",
        "\n",
        "        with open(complete_path_metadata, 'r', encoding='utf-8') as f:\n",
        "            reader = csv.reader(f)\n",
        "            for _ in range(20):\n",
        "                next(reader)\n",
        "\n",
        "            for starting_time_and_chroma in reader:\n",
        "                del starting_time_and_chroma[0]\n",
        "                chroma_and_start_time[str(folder_num)].append(\n",
        "                    list(map(float, starting_time_and_chroma)))  # string을 float으로 저장\n",
        "\n",
        "        with open(complete_path_annotations, 'r', encoding='utf-8') as f:\n",
        "            reader = csv.reader(f)\n",
        "            for interval_time_and_chord in reader:\n",
        "                if (interval_time_and_chord != []):\n",
        "                    chord_data_list = interval_time_and_chord[0].split('\\t')\n",
        "                    chord_data_list[2] = chord_to_int(chord_data_list[2])  # 코드이름\n",
        "                    chord_data_list[1] = float(chord_data_list[1])\n",
        "                    chord_data_list[0] = float(chord_data_list[0])\n",
        "                    chord_and_interval_time[str(folder_num)].append(chord_data_list)\n",
        "\n",
        "    print(len(chord_and_interval_time))\n",
        "    print(len(chroma_and_start_time))\n",
        "\n",
        "    for key, value in chroma_and_start_time.items():\n",
        "\n",
        "        for chord_list in chord_and_interval_time[key]:\n",
        "            chroma_bundle = []\n",
        "            num_flag = 0\n",
        "            for chroma_list in value:\n",
        "                # print(f'chroma_list[0] : {chroma_list[0]}')\n",
        "                # print(f'chord_list[0] : {chord_list[0]}')\n",
        "                # print(f'chord_list[1] : {chord_list[1]}')\n",
        "                if num_flag >= 30:\n",
        "                    break\n",
        "                if chroma_list[0] >= chord_list[1]:\n",
        "                    break\n",
        "                if chroma_list[0] >= chord_list[0] and chroma_list[0] < chord_list[1]:\n",
        "                    del chroma_list[0]\n",
        "                    chroma_bundle.append(chroma_list)\n",
        "                    # print(f'append {chroma_list}')\n",
        "                    value.remove(chroma_list)\n",
        "                    num_flag += 1\n",
        "\n",
        "            if chroma_bundle != []:\n",
        "                for i in range(30 - num_flag):\n",
        "                    chroma_bundle.append(chroma_bundle[-1])\n",
        "                if len(chroma_bundle) != 30:\n",
        "                    print(\"문제 발생!!!\")\n",
        "                data[\"Chromagram_bundles\"].append(chroma_bundle)\n",
        "                data[\"labels\"].append(chord_list[2])\n",
        "\n",
        "    print(len(data[\"labels\"]))\n",
        "    print(data[\"Chromagram_bundles\"][0])\n",
        "    print(len(data[\"Chromagram_bundles\"]))\n",
        "\n",
        "    print(len(data[\"Chromagram_bundles\"]))\n",
        "    print(len(data[\"Chromagram_bundles\"][0]))\n",
        "    print(len(data[\"Chromagram_bundles\"][0][0]))\n",
        "\n",
        "    x_list = data[\"Chromagram_bundles\"]\n",
        "    X = np.array(x_list)\n",
        "    y = np.array(data[\"labels\"])\n",
        "\n",
        "    print(X.shape)\n",
        "    # numpy array의 shape가 분명 (N, 30, 24)로 나와야 하는데 (N, )로 나오는 경우때문에 조금 헤맸다.\n",
        "    # 이 경우는 데이터의 크기가 30,24를 벗어나 있는 경우가 존재한다는 뜻이므로 데이터를 다시 재구성해주어야 한다.\n",
        "    print(y.shape)\n",
        "\n",
        "    with open(json_path, \"w\") as fp:\n",
        "        json.dump(data, fp, indent=4)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    prepare_dataset(DATASET_PATH, JSON_PATH)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dg3UDLpNAFlm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#### train model\n",
        "\n",
        "\n",
        "import json\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow.keras as keras\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "#Chromagram (# time steps, # of coefficient)\n",
        "\n",
        "\n",
        "JSON_PATH = 'data.json'\n",
        "SAVED_MODEL_PATH = \"model_256.h5\"\n",
        "\n",
        "\n",
        "def load_dataset(data_path):\n",
        "    with open(data_path, 'r') as fp:\n",
        "        data = json.load(fp)\n",
        "\n",
        "    # extract inputs and targets\n",
        "\n",
        "    X_array = np.array(data[\"Chromagram_bundles\"])\n",
        "    y_array = np.array(data[\"labels\"])\n",
        "\n",
        "    return X_array,y_array\n",
        "\n",
        "\n",
        "def plot_history(history):\n",
        "    \"\"\"Plots accuracy/loss for training/validation set as a function of the epochs\n",
        "        :param history: Training history of model\n",
        "        :return:\n",
        "    \"\"\"\n",
        "\n",
        "    fig, axs = plt.subplots(2)\n",
        "\n",
        "    # create accuracy sublpot\n",
        "    axs[0].plot(history.history[\"accuracy\"], label=\"train accuracy\")\n",
        "    axs[0].plot(history.history[\"val_accuracy\"], label=\"test accuracy\")\n",
        "    axs[0].set_ylabel(\"Accuracy\")\n",
        "    axs[0].legend(loc=\"lower right\")\n",
        "    axs[0].set_title(\"Accuracy eval\")\n",
        "\n",
        "    # create error sublpot\n",
        "    axs[1].plot(history.history[\"loss\"], label=\"train error\")\n",
        "    axs[1].plot(history.history[\"val_loss\"], label=\"test error\")\n",
        "    axs[1].set_ylabel(\"Error\")\n",
        "    axs[1].set_xlabel(\"Epoch\")\n",
        "    axs[1].legend(loc=\"upper right\")\n",
        "    axs[1].set_title(\"Error eval\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "def prepare_datasets(test_size, validation_size):\n",
        "    # load data\n",
        "    X, y = load_dataset(JSON_PATH)\n",
        "    print(\"How many dataset : \")\n",
        "    print(len(X))\n",
        "    print(len(y))\n",
        "\n",
        "    # create train/test split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size)\n",
        "\n",
        "    # create train/validation split\n",
        "    X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, test_size=validation_size)\n",
        "\n",
        "    # cnn과 달리 rnn에서는 이러한 3rd 차원이 필요 없다.\n",
        "    # X_train = X_train[..., np.newaxis] # 3d array -> (num_samples = 130, 13, 1)\n",
        "    # X_validation = X_validation[..., np.newaxis] # ... : 기존의 것들\n",
        "    # X_test = X_test[..., np.newaxis]\n",
        "\n",
        "    return X_train, X_validation, X_test, y_train, y_validation, y_test\n",
        "\n",
        "\n",
        "def build_model(input_shape):\n",
        "    # create RNN model\n",
        "    model = keras.Sequential()\n",
        "\n",
        "    # 2 LSTM layers\n",
        "    model.add(keras.layers.LSTM(128, input_shape=input_shape, return_sequences=True))\n",
        "    # return_sequences : second lstm에서 이 시퀀스를 사용하고 싶기 떄문에 true로 한다.\n",
        "    # model.add(keras.layers.LSTM(64, input_shape=input_shape, return_sequences=True)) #레이어를 하나 더 추가하는 것은 큰 효용이 없었다.\n",
        "    model.add(keras.layers.LSTM(128))\n",
        "\n",
        "    # dense layer\n",
        "    model.add(keras.layers.Dense(128, activation='relu'))\n",
        "    model.add(keras.layers.Dropout(0.3))\n",
        "\n",
        "    # output layer\n",
        "    model.add(keras.layers.Dense(106, activation='softmax'))\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def predict(model, X, y):\n",
        "    X = X[np.newaxis, ...]\n",
        "\n",
        "    # prediction = [[0.1, 0.2, ...]] # softmax의 결과물\n",
        "    prediction = model.predict(X)  # X -> (1, 130, 13, 1)\n",
        "\n",
        "    # extract index with max value\n",
        "    predicted_index = np.argmax(prediction, axis=1)  # [4]\n",
        "    print(\"Expected index: {}, Predicted index: {}\".format(y, predicted_index))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    pass\n",
        "    # create train, validation and test sets\n",
        "    X_train, X_validation, X_test, y_train, y_validation, y_test = prepare_datasets(0.25, 0.2)\n",
        "\n",
        "    input_shape = (X_train.shape[1], X_train.shape[2])  # (130, 13) (number of slices extract mfccs, mfccs) // 여기의 경우에서는 지금은\n",
        "\n",
        "    print(input_shape)\n",
        "    model = build_model(input_shape)\n",
        "\n",
        "    # compile model\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=0.01)\n",
        "    model.compile(optimizer=optimizer,\n",
        "                  loss=\"sparse_categorical_crossentropy\",\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    # train model\n",
        "    history = model.fit(X_train, y_train, validation_data=(X_validation, y_validation), batch_size=32, epochs=10)\n",
        "\n",
        "    # plot accuracy/error for training and validation\n",
        "    plot_history(history)\n",
        "\n",
        "    # evaluate model on test set\n",
        "    test_error, test_accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
        "    print(\"Accuracy on test set is: {}\".format(test_accuracy))\n",
        "\n",
        "    # make prediction on a sample\n",
        "    X = X_test[100]\n",
        "    y = y_test[100]\n",
        "\n",
        "    predict(model, X, y)\n",
        "\n",
        "    model.save(SAVED_MODEL_PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DvvqNg4AUT4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#### audio classification sevice class\n",
        "\n",
        "import tensorflow.keras as keras\n",
        "import numpy as np\n",
        "import librosa\n",
        "\n",
        "MODEL_PATH = \"model.h5\"\n",
        "NUM_SAMPLES_TO_CONSIDER = 22050 # = 1 sec\n",
        "SEGMENTATION_INTERVAL = 0.365\n",
        "\n",
        "class _Keyword_Spotting_Service:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    #singleton을 기초로 만든다....!\n",
        "\n",
        "    model = None\n",
        "    _instance = None\n",
        "    _mappings = []\n",
        "\n",
        "    type1 = ':maj'\n",
        "    type2 = ':min'\n",
        "    type3 = 'b:maj'\n",
        "    type4 = 'b:min'\n",
        "    type5 = '#:maj'\n",
        "    type6 = '#:min'\n",
        "    type7 = ':maj7'\n",
        "    type8 = ':min7'\n",
        "    type9 = 'b:maj7'\n",
        "    type10 = 'b:min7'\n",
        "    type11 = '#:maj7'\n",
        "    type12 = '#:min7'\n",
        "    type13 = ':7'\n",
        "    type14 = 'b:7'\n",
        "    type15 = '#:7'\n",
        "    type_table = [type1, type2, type3, type4, type5, type6, type7, type8, type9, type10, type11, type12, type13, type14,\n",
        "                  type15]\n",
        "\n",
        "    root_chord_list = ['A', 'B', 'C', 'D', 'E', 'F', 'G']\n",
        "\n",
        "\n",
        "    for chord_num, root_chord in enumerate(root_chord_list):\n",
        "        for type_num, type in enumerate(type_table):\n",
        "            chord_name = root_chord + type\n",
        "            _mappings.append(chord_name)\n",
        "    _mappings.append('None')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def predict(self, file_path):\n",
        "\n",
        "        # extract MFCCs\n",
        "        chromagram_list = self.preprocess(file_path) # (# segments, # coefficients)  (# : number of를 의미)\n",
        "\n",
        "        # convert 2d chromagram array into 3d array -> (# samples, # segments, # coefficients)\n",
        "        # make prediction\n",
        "\n",
        "        prediction_list = []\n",
        "        for chromagram in chromagram_list:\n",
        "            chromagram = chromagram[np.newaxis, ...]\n",
        "            predictions = self.model.predict(chromagram)  # [  [0.1, 0.6, 0.1, ...]  ]\n",
        "            predicted_index = np.argmax(predictions)\n",
        "            predicted_chord = self._mappings[predicted_index]\n",
        "            prediction_list.append(predicted_chord)\n",
        "\n",
        "        return prediction_list\n",
        "\n",
        "\n",
        "    def preprocess(self, file_path, n_chroma=24, n_fft=2048, hop_length=760): # hop length 380으로 30,24의 input shape 맞춰줌\n",
        "\n",
        "        num_samples_to_consider = int(SEGMENTATION_INTERVAL * NUM_SAMPLES_TO_CONSIDER)\n",
        "\n",
        "        segmentation_list = []\n",
        "\n",
        "        # load audio file\n",
        "        signal, sr = librosa.load(file_path)\n",
        "\n",
        "        # ensure consistency in the audio file length\n",
        "        length = 1\n",
        "\n",
        "        while(len(signal) > length * num_samples_to_consider):\n",
        "\n",
        "            temp_signal = signal[num_samples_to_consider * (length - 1):num_samples_to_consider * length]\n",
        "            segmentation_list.append(temp_signal)\n",
        "            length += 1\n",
        "\n",
        "        # extract chromagram\n",
        "\n",
        "        chromagram_list = []\n",
        "        for segment in segmentation_list:\n",
        "            chromagram = librosa.feature.chroma_stft(segment, n_chroma=n_chroma, n_fft=n_fft, hop_length=int(hop_length*SEGMENTATION_INTERVAL))\n",
        "            chromagram = chromagram.T\n",
        "            chromagram_list.append(chromagram)\n",
        "\n",
        "\n",
        "\n",
        "        return chromagram_list\n",
        "\n",
        "\n",
        "def Keyword_Sporring_Service():\n",
        "\n",
        "    #ensure that we only have 1 instance of KSS\n",
        "    if _Keyword_Spotting_Service._instance is None:\n",
        "        _Keyword_Spotting_Service._instance = _Keyword_Spotting_Service()\n",
        "        _Keyword_Spotting_Service.model = keras.models.load_model(MODEL_PATH)\n",
        "    return _Keyword_Spotting_Service._instance\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    kss = Keyword_Sporring_Service()\n",
        "\n",
        "    chord_list = kss.predict(\"test/Myself_edited.wav\")\n",
        "\n",
        "    for chord in chord_list:\n",
        "        print(f'Predicted keywords : {chord}')\n",
        "\n",
        "    none_num = 0\n",
        "\n",
        "    for chord in chord_list:\n",
        "        if chord == 'None':\n",
        "            none_num += 1\n",
        "\n",
        "    print(none_num)\n",
        "    print(none_num/len(chord_list))\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}